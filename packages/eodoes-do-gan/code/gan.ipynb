{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "style_transfer_e_5.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rF2x3qooyBTI"
      },
      "source": [
        "# STYLE\n",
        "\n",
        "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/style_transfer.ipynb\n",
        "\n",
        "\n",
        "https://github.com/manicman1999/Style-Transfer-TF2.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FLhN-gvebfo2",
        "colab": {}
      },
      "source": [
        "REGET = 0 # get again data\n",
        "GDRIVE = 1 # mount gdrive: gdata, gwork\n",
        "ING = 1 # ckpt in gwork\n",
        "CLEARTMP = 0 # clear g tmp folder \n",
        "MODITEM = \"\" # will look into module"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g5RstiiB8V-z",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x \n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "tf.__version__\n",
        "\n",
        "import IPython.display as display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12,12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import numpy as np\n",
        "import PIL\n",
        "import PIL.Image\n",
        "\n",
        "import time\n",
        "import functools\n",
        "import random\n",
        "\n",
        "import os\n",
        "\n",
        "import zipfile\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# To generate GIFs\n",
        "!pip install -q imageio\n",
        "import glob\n",
        "import imageio\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YX1q__WmZMrK"
      },
      "source": [
        "# GDATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1TK5PetzZOS5",
        "colab": {}
      },
      "source": [
        "# mount gdrive and set gdata\n",
        "import os\n",
        "\n",
        "# gdrive\n",
        "try:\n",
        "    GDRIVE\n",
        "except NameError:\n",
        "    GDRIVE = 1 # if not exist assume want to mount gdrive\n",
        "\n",
        "if GDRIVE:\n",
        "    from google.colab import drive # drive from colab\n",
        "    drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "gdrive = '/content/gdrive/My Drive'\n",
        "if not os.path.exists(gdrive):        \n",
        "    print(\"NO gdrive\")\n",
        "\n",
        "# gdata\n",
        "gdata = ''\n",
        "if os.path.exists(gdrive):        \n",
        "    gdata = '/content/gdrive/My Drive/gdata/' \n",
        "    print ('gdata is %s\\n' %gdata)\n",
        "\n",
        "    if os.path.exists(gdata):        \n",
        "        !ln -s \"$gdata\" /content/gdata\n",
        "else:\n",
        "    gdata = '/content/gdata/' \n",
        "    print ('gdata is %s\\n' %gdata)\n",
        "\n",
        "if not os.path.exists(gdata):\n",
        "    os.mkdir(gdata)\n",
        "\n",
        "# gwork\n",
        "gwork = ''\n",
        "if os.path.exists(gdrive):        \n",
        "    gwork = '/content/gdrive/My Drive/gwork/' \n",
        "    print ('gwork is %s\\n' %gwork)\n",
        "\n",
        "    if os.path.exists(gwork):        \n",
        "        !ln -s \"$gwork\" /content/gwork\n",
        "else:\n",
        "    gwork = '/content/gwork/' \n",
        "\n",
        "if not os.path.exists(gwork):\n",
        "    os.mkdir(gwork)\n",
        "print ('gwork is %s\\n' %gwork)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS2Dk5xkIN8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clear work area\n",
        "try:\n",
        "    CLEARTMP\n",
        "except NameError:\n",
        "    CLEARTMP = 0 # if not exist assume not to clear\n",
        "\n",
        "if CLEARTMP:\n",
        "    for root, dirs, files in os.walk(gwork, topdown=False):\n",
        "\n",
        "        print(\"clear %s\" %root)\n",
        "        for name in files:\n",
        "            os.remove(os.path.join(root, name))\n",
        "        for name in dirs:\n",
        "            os.rmdir(os.path.join(root, name))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1qBFyTSC3R2F"
      },
      "source": [
        "# IMAGINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9JYZDxZK6BMH",
        "colab": {}
      },
      "source": [
        "# IMAGINE \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as display\n",
        "\n",
        "class IMAGINE(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 verbose = False):\n",
        "        self.verbose = verbose\n",
        "\n",
        "    # ******************\n",
        "    #  tensor_to_image\n",
        "    #\n",
        "    def tensor_to_image(self, tensor):\n",
        "        tensor = tensor*255\n",
        "        tensor = np.array(tensor, dtype=np.uint8)\n",
        "        if np.ndim(tensor)>3:\n",
        "            assert tensor.shape[0] == 1\n",
        "            tensor = tensor[0]\n",
        "        return PIL.Image.fromarray(tensor)\n",
        "\n",
        "    # ******************\n",
        "    #  imshow\n",
        "    #  tf.squeeze: removes dimensions of size 1 from the shape of a tensor\n",
        "    #\n",
        "    def imshow(self, image, title=None):\n",
        "        if len(image.shape) > 3:\n",
        "            image = tf.squeeze(image, axis=0)\n",
        "\n",
        "        plt.imshow(image)\n",
        "        if title:\n",
        "            plt.title(title)    \n",
        "\n",
        "    # ******************\n",
        "    #  high_pass_x_y\n",
        "    #   high frequency components of the image\n",
        "    #   ref: https://www.tensorflow.org/tutorials/generative/style_transfer\n",
        "    #\n",
        "    def high_pass_x_y(self, image):\n",
        "        x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n",
        "        y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n",
        "\n",
        "        return x_var, y_var\n",
        "\n",
        "    # ******************\n",
        "    #  clip_0_1\n",
        "    #  function to keep the pixel values between 0 and 1\n",
        "    #\n",
        "    def clip_0_1(self, image):\n",
        "\n",
        "        return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)                \n",
        "\n",
        "    # ******************\n",
        "    #  random_crop\n",
        "    #\n",
        "    def random_crop(self, img, w=256, h=256):\n",
        "\n",
        "        cropped_image = tf.image.random_crop(\n",
        "            img, size=[w, h, 3])\n",
        "\n",
        "        return cropped_image\n",
        "\n",
        "    # ******************\n",
        "    #  normalize\n",
        "    #\n",
        "    # normalizing the images to [-1, 1]\n",
        "    #\n",
        "    def normalize(self, img):\n",
        "        img = tf.cast(img, tf.float32)\n",
        "        img = (img / 127.5) - 1\n",
        "\n",
        "        return img\n",
        "\n",
        "    # ******************\n",
        "    #  preprocess_image\n",
        "    #\n",
        "    def preprocess_image(self, img, w=256, h=256):\n",
        "\n",
        "        img = self.normalize(img)\n",
        "        img = tf.image.resize(img, [w, h])\n",
        "\n",
        "        return img\n",
        "\n",
        "    # ******************\n",
        "    #  preprocess\n",
        "    #\n",
        "    def preprocess(self, images, w=256, h=256, buffer_size=128):\n",
        "\n",
        "        images = images.map(\n",
        "            self.preprocess_image,\n",
        "            num_parallel_calls=tf.data.experimental.AUTOTUNED\n",
        "        ).cache().shuffle(buffer_size).batch(1)\n",
        "\n",
        "        return images\n",
        "\n",
        "\n",
        "    # ******************\n",
        "    #  load_img\n",
        "    #\n",
        "    def load_img(self, path_to_img, max_dim = 512):\n",
        "        \n",
        "        img = tf.io.read_file(path_to_img)\n",
        "        img = tf.image.decode_image(img, channels=3)\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "        shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "        long_dim = max(shape)\n",
        "        scale = max_dim / long_dim\n",
        "\n",
        "        new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "        img = tf.image.resize(img, new_shape)\n",
        "        img = img[tf.newaxis, :]\n",
        "        return img\n",
        "\n",
        "\n",
        "    # ******************\n",
        "    #   get_images\n",
        "    #\n",
        "    #   get image files from folder\n",
        "    #\n",
        "    def get_images(self, directory='dataset/'): # get images from local folder\n",
        "        jpgs = glob.glob( '{}*.jpg'.format(directory) )\n",
        "        pngs = glob.glob( '{}*.png'.format(directory) )\n",
        "        allimages = jpgs + pngs\n",
        "        return allimages\n",
        "\n",
        "    # ******************\n",
        "    #   load\n",
        "    #\n",
        "    def load(self, image_file):\n",
        "        image = tf.io.read_file(image_file)\n",
        "        image = tf.image.decode_jpeg(image)\n",
        "\n",
        "        w = tf.shape(image)[1]\n",
        "\n",
        "        w = w // 2\n",
        "        real_image = image[:, :w, :]\n",
        "        input_image = image[:, w:, :]\n",
        "\n",
        "        input_image = tf.cast(input_image, tf.float32)\n",
        "        real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "        return input_image, real_image\n",
        "\n",
        "    # ******************\n",
        "    #   random_jitter\n",
        "    #\n",
        "    def random_jitter(self, image):\n",
        "        # resizing to 286 x 286 x 3\n",
        "        image = tf.image.resize(image, [286, 286],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "        # randomly cropping to 256 x 256 x 3\n",
        "        image = random_crop(image)\n",
        "\n",
        "        # random mirroring\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "    # ******************\n",
        "    #  tiles\n",
        "    #\n",
        "    def tiles(self, images, rows = 4, cols = 4):\n",
        "        for i in range(len(images)):\n",
        "            plt.subplot(images, rows, i+1)\n",
        "            plt.title(i)\n",
        "            if i % cols == 0:\n",
        "                plt.imshow(images[i][0] * 0.5 + 0.5)\n",
        "            else:\n",
        "                plt.imshow(images[i][0] * 0.5 * contrast + 0.5)\n",
        "        plt.show()\n",
        "\n",
        "    # ******************\n",
        "    #  create_image\n",
        "    #\n",
        "    def create_image(self, images, name='create_img_name', xsize=4, ysize=4):\n",
        "        \n",
        "        fig, axs = plt.subplots(xsize, ysize, figsize=(xsize*2,ysize*2))\n",
        "        plt.subplots_adjust(left=0.05,bottom=0.05,right=0.95,top=0.95, wspace=0.2, hspace=0.2)\n",
        "\n",
        "        cnt = 0\n",
        "        for i in range(ysize):\n",
        "            for j in range(xsize):\n",
        "                axs[i,j].imshow(images[cnt])\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "\n",
        "        fig.savefig(name, facecolor='white' )      \n",
        "        plt.close()\n",
        "\n",
        "    # ******************\n",
        "    #  genimages tbd\n",
        "    #\n",
        "    def genimages(self, images, cols = 2):\n",
        "        sets =[images[i:i + cols] for i in range(0, len(images), cols)]\n",
        "        for s in sets:\n",
        "            for i in range(cols):\n",
        "                plt.subplot(1, cols, i+1)\n",
        "                plt.title(i)\n",
        "                plt.imshow(images[cols * s + i])\n",
        "                plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF0X3Z2n33y-",
        "colab_type": "text"
      },
      "source": [
        "# ANIME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvVYsCxK35aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ANIME\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as anim\n",
        " \n",
        "from skimage.transform import rotate \n",
        "from skimage.color import rgb2hsv, hsv2rgb\n",
        "\n",
        "from moviepy.editor import *\n",
        "from moviepy.Clip import Clip\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "from moviepy.video.VideoClip import VideoClip\n",
        "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
        "from moviepy.audio.io.AudioFileClip import AudioFileClip\n",
        "from moviepy.video.io.ffmpeg_reader import FFMPEG_VideoReader\n",
        "\n",
        "\n",
        "class ANIME():\n",
        "    def __init__(self, \n",
        "                 xSize=128, # dataset\n",
        "                 ySize=128, # dataset\n",
        "                 nSlices=100, # dataset\n",
        "                 resize=1, # dataset\n",
        "                 directory='gainsborough/', # dataset\n",
        "                 name= 'gainsborough_256_128', # dataset\n",
        "                 latent_dim=256, # model\n",
        "                 range_plot=4, # plot\n",
        "                 epochs=101, # train\n",
        "                 batch_size=32, # train\n",
        "                 save_interval=50, # train\n",
        "                 channels=3,\n",
        "                 imgXsize = 512, # image\n",
        "                 imgYsize = 512, # image\n",
        "                 img_rows = 128, # image\n",
        "                 img_cols = 128, # image\n",
        "                 rows = 4, # mosaic\n",
        "                 columns = 4, # mosaic\n",
        "\n",
        "                 fps = 30, # video settings\n",
        "                 maxTime = 5, # 30 # seconds # video settings\n",
        "                 frameCount = 0, # video settings\n",
        "                 time = 0, # video settings\n",
        "\n",
        "                 ):\n",
        "        \n",
        "        # self.fig, self.axs = plt.subplots(rows,columns, figsize=(4,4), facecolor=(1,1,1) )\n",
        "        # plt.subplots_adjust(left=0.05,bottom=0.05,right=0.95,top=0.95, wspace=0.2, hspace=0.2)\n",
        "\n",
        "        # self.fig.set_size_inches(size[0] / 100, size[1] / 100)\n",
        "        self.images = []\n",
        "        self.size=(imgXsize, imgYsize)\n",
        "        self.ntales = rows * columns # tales in frame\n",
        "        self.maxTime = maxTime\n",
        "\n",
        "        self.nframes = int( maxTime*fps ), # video settings\n",
        "\n",
        "    #   *******************\n",
        "    #   add\n",
        "    #\n",
        "    #   add images to self.images\n",
        "    #\n",
        "    def add(self, images):\n",
        "        imgs = []\n",
        "        for k in range(16):\n",
        "            ax = self.axs[int(k/4.),k%4].imshow(images[k])\n",
        "            self.axs[int(k/4.),k%4].axis('off')\n",
        "            imgs.append(ax)\n",
        "        self.images.append(imgs)\n",
        "\n",
        "    #   *******************\n",
        "    #   save\n",
        "    #\n",
        "    #   save animation\n",
        "    #\n",
        "    def save(self, filename, fps=1):\n",
        "        animation = anim.ArtistAnimation(self.fig, self.images)\n",
        "        animation.save(filename, fps=fps,\n",
        "            #progress_callback = lambda i, n: print(f'Saving frame {i} of {n}')\n",
        "        )\n",
        "\n",
        "    #   *******************\n",
        "    #   anim\n",
        "    #\n",
        "    #   display.Image(filename=anim_file)\n",
        "    #\n",
        "    def anim(self, srcfolder, anim_file):\n",
        "        with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "            filenames = glob.glob(srcfolder + '*.jpg')\n",
        "            filenames = sorted(filenames)\n",
        "            last = -1\n",
        "\n",
        "            # ingsn = len(filenames) / 2\n",
        "            for i in range(len(filenames)):\n",
        "                plt.subplot(2, 2, i+1)\n",
        "                print(filenames[i])\n",
        "                image = Imagine.load_img(filenames[i])\n",
        "                plt.imshow(image[0])\n",
        "                plt.show()\n",
        "\n",
        "            for i,filename in enumerate(filenames):\n",
        "                print(filename)\n",
        "                img = Imagine.load_img(filename)\n",
        "                Imagine.imshow(img)\n",
        "                frame = 2*(i**0.5)\n",
        "                if round(frame) > round(last):\n",
        "                    last = frame\n",
        "                else:\n",
        "                    continue\n",
        "                image = imageio.imread(filename)\n",
        "\n",
        "            writer.append_data(image)\n",
        "            image = imageio.imread(filename)\n",
        "            writer.append_data(image)\n",
        "\n",
        "    #   *******************\n",
        "    #   make_fab\n",
        "    #\n",
        "    def make_fab(self, model):\n",
        "        \n",
        "        # model.load_weights(generator_file=\"generator ({}).h5\".format(model.name), \\\n",
        "        #                    discriminator_file=\"discriminator ({}).h5\".format(model.name))\n",
        "        model.load_weights(generator_file=\"{}.tf\".format(model.name),\n",
        "                           discriminator_file=\"{}.tf\".format(model.name))\n",
        "        # # video settings\n",
        "        # fps = fps\n",
        "        # maxTime = maxTime # seconds\n",
        "        # frameCount = frameCount\n",
        "        # time = time\n",
        "        # nframes = nframes\n",
        "\n",
        "        # controls for animation\n",
        "        seed_start = np.random.normal(1, 1, (16, model.latent_dim))\n",
        "        latentSpeed = np.random.normal(2, 1, (16, model.latent_dim))\n",
        "        vary = np.random.normal(1, 1, (16, nframes, model.latent_dim)) \n",
        "\n",
        "        # randomize image transformations\n",
        "        #rhue =  np.random.random()\n",
        "        #rotation = 360 * np.round(np.random.random((4,))*4)/4 # random rotation\n",
        "        #flip = np.random.randint(0,4,(4,)) # 0=normal, 1=y-axis, 2=x-axis, 3=transpose \n",
        "\n",
        "        # latent parameter animation\n",
        "        for k in range(16):\n",
        "            time = 0\n",
        "\n",
        "            # for each image in animation \n",
        "            for i in range(nframes): \n",
        "                \n",
        "                # change the latent variables\n",
        "                for j in range(model.latent_dim):\n",
        "                    vary[k][i][j] = seed_start[k][j] + np.sin( 2*np.pi*(time/maxTime) * latentSpeed[k][j] ) \n",
        "\n",
        "                time += 1./fps\n",
        "\n",
        "        imgs = []\n",
        "        for k in range(16):\n",
        "            imgs.append(\n",
        "                model.generator.predict(vary[k])\n",
        "            )\n",
        "        imgs = np.array(imgs)\n",
        "\n",
        "        # create animation\n",
        "        animated_gif = AnimatedGif()\n",
        "        for i in range(nframes):\n",
        "            \n",
        "            animated_gif.add(imgs[:,i])\n",
        "\n",
        "        animated_gif.save('artificial_art.mp4',fps=fps)\n",
        "\n",
        "        # dude() \n",
        "\n",
        "        count = np.loadtxt('count.txt')\n",
        "\n",
        "        with open('hashtags.txt') as fp:\n",
        "            hashtags = fp.readlines()\n",
        "        hashtags = [hashtags[i].strip() for i in range(len(hashtags))]\n",
        "\n",
        "        message = \"Automated Artificial Art v 1.{:.1f} - machine hallucinations from an artificial neural network \\n \\n#\".format(count[0])\n",
        "        message = message + \" #\".join( np.random.choice(hashtags,4))\n",
        "\n",
        "\n",
        "    # Display a single image using the epoch number\n",
        "\n",
        "    def display_image(self, epoch_no):\n",
        "        return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n",
        "\n",
        "    def display_lastimage(self):\n",
        "        trainer = self.trainer\n",
        "        epoch_no = trainer.epochs\n",
        "        return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n",
        "\n",
        "    #   ************\n",
        "    #   generate_gif\n",
        "    #\n",
        "    def generate_gif(self, \n",
        "                     srcfolder = None,\n",
        "                     dstfile = 'ani.gif',\n",
        "                     dstfolder = None,\n",
        "                     ):\n",
        "\n",
        "\n",
        "        print(\"srcfolder %s\" %srcfolder)\n",
        "        with imageio.get_writer(dstfile, mode='I') as writer:\n",
        "            if srcfolder:\n",
        "                imgfiles = srcfolder + '/' + '*.png'\n",
        "            else:\n",
        "                imgfiles = '*.png'\n",
        "\n",
        "            filenames = glob.glob(imgfiles)\n",
        "            filenames = sorted(filenames)\n",
        "\n",
        "            print(\"filenames %s\" %filenames)\n",
        "            last = -1\n",
        "            for i,filename in enumerate(filenames):\n",
        "                frame = 2*(i**0.5)\n",
        "                if round(frame) > round(last):\n",
        "                    last = frame\n",
        "                else:\n",
        "                    continue\n",
        "                image = imageio.imread(filename)\n",
        "                writer.append_data(image)\n",
        "\n",
        "        import IPython\n",
        "        if IPython.version_info >= (6,2,0,''):\n",
        "            display.Image(filename=dstfile)\n",
        "\n",
        "    #   ************\n",
        "    #   display_gif\n",
        "    #\n",
        "    def display_gif(self, giffile = 'ani.gif', height=256, autoplay=0, loop=1):\n",
        "\n",
        "        vids = [giffile]\n",
        "        for i, vid_name in enumerate(vids):\n",
        "            vid_path = '%s' %vid_name\n",
        "            print (\"vid_path %s\" %vid_path)\n",
        "            clip = VideoFileClip(vid_path)\n",
        "            display(clip.ipython_display(height, autoplay, loop))    \n",
        "\n",
        "    #   ************\n",
        "    #   display_history\n",
        "    #\n",
        "    def display_history(self):\n",
        "\n",
        "        acc = history.history['acc']\n",
        "        val_acc = history.history['val_acc']\n",
        "        loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "\n",
        "        epochs = range(len(acc))\n",
        "\n",
        "        plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "        plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "        plt.title('Training and validation accuracy')\n",
        "        plt.legend(loc=0)\n",
        "        plt.figure()\n",
        "\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hl8j5AJMruN",
        "colab_type": "text"
      },
      "source": [
        "# EXCHECKER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xtKBXssI4M-Q",
        "colab": {}
      },
      "source": [
        "#   EXCHECKER\n",
        "#\n",
        "#   Manage model state\n",
        "#\n",
        "try:\n",
        "    gwork\n",
        "except NameError:|\n",
        "    gwork = ''\n",
        "\n",
        "class EXCHECKER(tf.keras.models.Model):\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 ckptpath = 'training_checkpoints',\n",
        "                 clear = True\n",
        "                 ):\n",
        "        \n",
        "        super(EXCHECKER, self).__init__()\n",
        "        \n",
        "        self.clear = clear\n",
        "        self.model = model\n",
        "        self.ckptpath = ckptpath\n",
        "        self.step = 1\n",
        "\n",
        "    #   ******************\n",
        "    #   ckpt_sample\n",
        "    #\n",
        "    #\n",
        "    def ckpt_sample(self):\n",
        "        ckpt = tf.train.Checkpoint(v=tf.Variable(0.))\n",
        "        mgr = tf.train.CheckpointManager(ckpt, '/tmp/tfckpts', max_to_keep=5)\n",
        "\n",
        "        @tf.function\n",
        "        def train_fn():\n",
        "            data = tf.data.Dataset.range(10)\n",
        "            for item in data:\n",
        "                ckpt.v.assign_add(tf.cast(item, tf.float32))\n",
        "                tf.py_function(mgr.save, [], [tf.string])\n",
        "\n",
        "        train_fn()\n",
        "\n",
        "\n",
        "    #   ******************\n",
        "    #   get_ckpt_prefix\n",
        "    #\n",
        "    #\n",
        "    def get_ckpt_prefix(self):\n",
        "        ckptpath = self.ckptpath\n",
        "\n",
        "        return ckptpath\n",
        "\n",
        "    #   ******************\n",
        "    #   get_checkpoint\n",
        "    #\n",
        "    #\n",
        "    def get_checkpoint(self):\n",
        "        print(\"exchecker.get_checkpoint\")\n",
        "        model = self.model\n",
        "        p = model.chkpt\n",
        "\n",
        "        ckpt = tf.train.Checkpoint(**p)\n",
        "        return ckpt\n",
        "\n",
        "    #   ******************\n",
        "    #   get_manager\n",
        "    #\n",
        "    #\n",
        "    def get_manager(self, ckpt):\n",
        "        model = self.model\n",
        "        ckptpath = model.ckptpath\n",
        "\n",
        "        if self.clear:\n",
        "            if os.path.exists(ckptpath):\n",
        "                for root, dirs, files in os.walk(ckptpath, topdown=False):\n",
        "                    print(\"clear ckpt dir: %s\" %root)\n",
        "                    for name in files:\n",
        "                        os.remove(os.path.join(root, name))\n",
        "                    for name in dirs:\n",
        "                        os.rmdir(os.path.join(root, name))\n",
        "                \n",
        "        print(\"exchecker get_manager\")\n",
        "        max_to_keep = 3\n",
        "        manager = tf.train.CheckpointManager(ckpt, \n",
        "                                             ckptpath, \n",
        "                                             max_to_keep)\n",
        "        print(\"exchecker got manager\")        \n",
        "        return manager\n",
        "\n",
        "    #   ******************\n",
        "    #   restore\n",
        "    #\n",
        "    #   if a checkpoint exists, restore the latest checkpoint\n",
        "    #   eg:\n",
        "    #       ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "    #       if ckpt_manager.latest_checkpoint:\n",
        "    #           ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    #           print ('Latest checkpoint restored!!')\n",
        "    #\n",
        "    def restore(self, ckpt):\n",
        "        model = self.model\n",
        "        ckptpath = self.ckptpath\n",
        "         \n",
        "        print(\"checkpoint restore %s\" %ckpt)\n",
        "        res = ckpt.restore(tf.train.latest_checkpoint(ckptpath))\n",
        "        print(\"res: in % s checkpoint: %s\" %(ckptpath, res))\n",
        "\n",
        "\n",
        "    # checkpoint_path = \"./checkpoints/train\"\n",
        "    # ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
        "    #                            generator_f=generator_f,\n",
        "    #                            discriminator_x=discriminator_x,\n",
        "    #                            discriminator_y=discriminator_y,\n",
        "    #                            generator_g_optimizer=generator_g_optimizer,\n",
        "    #                            generator_f_optimizer=generator_f_optimizer,\n",
        "    #                            discriminator_x_optimizer=discriminator_x_optimizer,\n",
        "    #                            discriminator_y_optimizer=discriminator_y_optimizer)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dI6yEPspzHm7"
      },
      "source": [
        "# DATAGEN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UCkaDFq9zJQ8",
        "colab": {}
      },
      "source": [
        "#   DATAGEN\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.python.keras.utils.data_utils import get_file\n",
        "from tensorflow.python.util.tf_export import keras_export\n",
        "\n",
        "from shutil import copyfile\n",
        "from shutil import copy\n",
        "\n",
        "class DATAGEN():\n",
        "    def __init__(self, \n",
        "                 name='sol',\n",
        "                 buffer_size = 60000,\n",
        "                 batch_size = 256):\n",
        "\n",
        "\n",
        "        self.name = name\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    #   ******************\n",
        "    #   clear_folder\n",
        "    #\n",
        "    #\n",
        "    def clear_folder(self, dir):\n",
        "\n",
        "        if os.path.exists(dir):\n",
        "            for root, dirs, files in os.walk(dir, topdown=False):\n",
        "                print(\"clear %s\" %root)\n",
        "                for name in files:\n",
        "                    os.remove(os.path.join(root, name))\n",
        "                for name in dirs:\n",
        "                    os.rmdir(os.path.join(root, name))\n",
        "                \n",
        "    # ******************\n",
        "    #  get_uriitems\n",
        "    #\n",
        "    #  get remote data \n",
        "    #   urd:    remote url srcfolder\n",
        "    #   urns:   remote url src files\n",
        "    #   gdir:   persistent folder\n",
        "    #   wdir:   work folder\n",
        "    #\n",
        "    def get_uriitems(self, urd, urls, gdir, wdir, verbose = False):\n",
        "\n",
        "        if 1: # if g src eq g raw, download to raw\n",
        "            if not os.path.exists(wdir):\n",
        "                if verbose:\n",
        "                    print(\"create wdir: %s\" %wdir)\n",
        "                os.mkdir(wdir)\n",
        "        \n",
        "            if not os.path.exists(gdir):\n",
        "                os.mkdir(gdir)\n",
        "                if verbose:\n",
        "                    print(\"create gdir %s and download urls\" %gdir)\n",
        "\n",
        "                for img in urls:\n",
        "                    uri = urd + img\n",
        "                    !wget --no-check-certificate $uri -P  $gdir\n",
        "\n",
        "            if os.path.exists(gdir):\n",
        "                for file in urls:\n",
        "                    dstfile = os.path.join(wdir, file)\n",
        "                    srcfile = os.path.join(gdir, file)\n",
        "\n",
        "                    if os.path.exists(srcfile):\n",
        "                        if verbose:\n",
        "                            print(\"file %s in gdir\" %(srcfile))\n",
        "                        if not os.path.exists(dstfile):\n",
        "                            if verbose:\n",
        "                                print(\"copy cp %s to %s\" %(srcfile, dstfile))\n",
        "                            copy(srcfile, dstfile)\n",
        "\n",
        "                    else:\n",
        "                        print(\"file %s not available in gdir\" %srcfile)\n",
        "                        uri = urd + file\n",
        "                        if verbose:\n",
        "                            print(\"download from uri %s\" %file)\n",
        "                        !wget --no-check-certificate $uri -P  \"$gdir\"\n",
        "                        if verbose:\n",
        "                            print(\"copy %s to %s\" %(srcfile, dstfile))\n",
        "                        copy(srcfile, dstfile)\n",
        "\n",
        "\n",
        "    # ******************\n",
        "    #  get_rawdata\n",
        "    #\n",
        "    #  get remote data \n",
        "    #   urd:    remote url srcfolder\n",
        "    #   urns:   remote url src files\n",
        "    #   path:   local destination path\n",
        "    #\n",
        "    def get_rawdata(self, path, urd, urns):\n",
        "        directory = path\n",
        "\n",
        "        if not os.path.exists(directory):\n",
        "            print(\"create imgfolder: %s\" %directory)\n",
        "            os.mkdir(directory)\n",
        "\n",
        "        for img in urns:\n",
        "            print(img)\n",
        "            uri = urd + img\n",
        "            !wget --no-check-certificate $uri -P  $directory\n",
        "\n",
        "\n",
        "    # ******************\n",
        "    #  get_gdata\n",
        "    #\n",
        "    #  get data from gdata \n",
        "    #   gpath:  gdata folder\n",
        "    #   wpath:  work folder\n",
        "    #\n",
        "    def get_gdata(self, gpath, wpath, fnames):\n",
        "        if os.path.exists(gpath):\n",
        "            for file in fnames:\n",
        "                src = os.path.join(gpath, file)\n",
        "                dst = os.path.join(wpath, file)\n",
        "                dstdir = wpath\n",
        "                # https://docs.python.org/2/library/shutil.html\n",
        "                print(\"cp %s to %s\" %(src, dstdir))\n",
        "                copy(src, dstdir)\n",
        "        else:\n",
        "            print(\"gsrc does not exist\")        \n",
        "\n",
        "\n",
        "    # ******************\n",
        "    #  get_gfolder\n",
        "    #\n",
        "    #  get data from gdata \n",
        "    #   gpath:  gdata folder\n",
        "    #   wpath:  work folder\n",
        "    #\n",
        "    def get_gfolder(self, gpath, wpath):\n",
        "        if os.path.exists(gpath):\n",
        "            for base, dirs, files in os.walk(gpath):\n",
        "                for file in files:\n",
        "                    src = os.path.join(base, file)\n",
        "                    dst = os.path.join(wpath, file)\n",
        "                    dstdir = wpath\n",
        "                    # https://docs.python.org/2/library/shutil.html\n",
        "                    print(\"cp %s to %s\" %(src, dstdir))\n",
        "                    copy(src, dstdir)\n",
        "        else:\n",
        "            print(\"gsrc does not exist\")        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # ******************\n",
        "    #   get_images\n",
        "    #\n",
        "    #   get images from local folder\n",
        "    #\n",
        "    def get_images(self, path): \n",
        "        directory = path\n",
        "        jpgs = glob.glob( '{}*.jpg'.format(directory) )\n",
        "        pngs = glob.glob( '{}*.png'.format(directory) )\n",
        "        allimages = jpgs + pngs\n",
        "        return allimages\n",
        "\n",
        "    #   ******************\n",
        "    #   get_dataset\n",
        "    #\n",
        "    def get_dataset(self, \n",
        "                    path = \"\",\n",
        "                    buffer_size = 60000,\n",
        "                    batch_size = -1,                    \n",
        "                    xSize=128, \n",
        "                    ySize=128, \n",
        "                    nSlices=100, \n",
        "                    resize=0.75, \n",
        "                    channels=3):\n",
        "        \n",
        "        allimages = self.get_images(path)\n",
        "\n",
        "        x_train = []\n",
        "        y_train = []\n",
        "\n",
        "        nimages = len(allimages)\n",
        "        print(\"n images: %s\" %nimages)\n",
        "\n",
        "        for i in range(len(allimages)):\n",
        "\n",
        "            # load image\n",
        "            img = Image.open(allimages[i])\n",
        "\n",
        "            if resize != 1:\n",
        "                # resizes image in-place\n",
        "                img.thumbnail((img.size[0]*resize, img.size[1]*resize), Image.LANCZOS) \n",
        "\n",
        "            img_data = np.array(list(img.getdata())).reshape( (img.size[1],img.size[0],-1) ) \n",
        "            \n",
        "            for n in range(nSlices):\n",
        "\n",
        "                # create random slices \n",
        "                rx = np.random.randint( img.size[0]-xSize)\n",
        "                ry = np.random.randint( img.size[1]-ySize)\n",
        "            \n",
        "                # pull out portion of ccd\n",
        "                sub = np.copy(img_data[ry:ry+ySize, rx:rx+xSize]).astype(float)\n",
        "\n",
        "                #x_train.append( preprocessing.scale(sub) )\n",
        "                x_train.append( sub[:,:,:3]  ) \n",
        "                y_train.append( [rx,ry] )\n",
        "\n",
        "        x_train = np.array(x_train)\n",
        "        y_train = np.array(y_train)\n",
        "\n",
        "        # return (x_train, y_train)\n",
        "\n",
        "        print(\"x_train.shape: \", x_train.shape)\n",
        "\n",
        "        x_train /= 255\n",
        "        sliced_images =  tf.data.Dataset.from_tensor_slices(x_train)\n",
        "        shuffled_images = sliced_images.shuffle(buffer_size)\n",
        "        if batch_size > 0:\n",
        "            batched_images = shuffled_images.batch(batch_size)\n",
        "        else:\n",
        "            batched_images = shuffled_images\n",
        "        train_dataset = batched_images\n",
        "\n",
        "        return train_dataset\n",
        "\n",
        "\n",
        "\n",
        "    # ******************\n",
        "    #   load_data\n",
        "    #       'untar': False,\n",
        "    #       'md5_hash': None,\n",
        "    #       'file_hash': None,\n",
        "    #       'cache_subdir': 'datasets',\n",
        "    #       'hash_algorithm': 'auto',\n",
        "    #       'extract': False,\n",
        "    #       'archive_format': 'auto',\n",
        "    #       'cache_dir': None       \n",
        "    #\n",
        "    def load_data(self, path='mnist.npz', urd=None, urf=None):\n",
        "\n",
        "      path = get_file(\n",
        "          path,\n",
        "          origin=urd + urf)\n",
        "      \n",
        "      with np.load(path) as f:\n",
        "        x_train, y_train = f['x_train'], f['y_train']\n",
        "        x_test, y_test = f['x_test'], f['y_test']\n",
        "\n",
        "        return (x_train, y_train), (x_test, y_test) \n",
        "\n",
        "\n",
        "    # ******************\n",
        "    #   download_file\n",
        "    # \n",
        "    def download_file(self, **kwargs):\n",
        "        # Finally, save the result\n",
        "\n",
        "        file_name = 'stylized-image.png'\n",
        "        Imagine.tensor_to_image(image).save(file_name)\n",
        "\n",
        "        try:\n",
        "            from google.colab import files\n",
        "        except ImportError:\n",
        "            pass\n",
        "        else:\n",
        "            files.download(file_name)\n",
        "\n",
        "    # ******************\n",
        "    #   create_dataset_np\n",
        "    # \n",
        "    def create_dataset_np(self, xSize=128, ySize=128, nSlices=100, resize=0.75, directory='dataset/'):\n",
        "        jpgs = glob.glob( '{}*.jpg'.format(directory) )\n",
        "        pngs = glob.glob( '{}*.png'.format(directory) )\n",
        "\n",
        "        allimages = jpgs + pngs\n",
        "\n",
        "        x_train = []\n",
        "        y_train = []\n",
        "\n",
        "        for i in range(len(allimages)):\n",
        "\n",
        "            # load image\n",
        "            img = Image.open(allimages[i])\n",
        "\n",
        "            if resize != 1:\n",
        "                img.thumbnail((img.size[0]*resize, img.size[1]*resize), Image.LANCZOS) # resizes image in-place\n",
        "\n",
        "            img_data = np.array(list(img.getdata())).reshape( (img.size[1],img.size[0],-1) ) \n",
        "\n",
        "            for n in range(nSlices):\n",
        "\n",
        "                # create random slices \n",
        "                rx = np.random.randint( img.size[0]-xSize)\n",
        "                ry = np.random.randint( img.size[1]-ySize)\n",
        "            \n",
        "                # pull out portion of ccd\n",
        "                sub = np.copy(img_data[ry:ry+ySize, rx:rx+xSize]).astype(float)\n",
        "\n",
        "                #x_train.append( preprocessing.scale(sub) )\n",
        "                x_train.append( sub[:,:,:3]  ) \n",
        "                y_train.append( [rx,ry] )\n",
        "\n",
        "        x_train = np.array(x_train)\n",
        "        y_train = np.array(y_train)\n",
        "        \n",
        "        x_train /= 255 \n",
        "\n",
        "        return (x_train, y_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec57LRDWkCx2",
        "colab_type": "text"
      },
      "source": [
        "# === MATCHUE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdw3iCWRBIAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -la \"/content/gdrive/My Drive/gwork/flower_photos.tar.gz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT_tdnPebrZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 1:\n",
        "    #Import packages\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    from tensorflow.keras.layers import *\n",
        "    from tensorflow.keras.models import *\n",
        "    from tensorflow.keras.datasets import mnist\n",
        "    from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "    #   MATCHUE\n",
        "    #\n",
        "    class MATCHUE(tf.keras.models.Model):\n",
        "\n",
        "        def __init__(self,\n",
        "                    image = None,\n",
        "                    verbose = True,\n",
        "                    ):\n",
        "\n",
        "            super(MATCHUE, self).__init__()\n",
        "\n",
        "            self.image = image\n",
        "\n",
        "            self.qepoch = tf.Variable(0.) # _e_      \n",
        "            self.qepoch.assign_add(tf.cast(1, tf.float32))\n",
        "            print(\"qepoch: %s\" %(self.qepoch.numpy()))\n",
        "\n",
        "    matchue = MATCHUE()\n",
        "\n",
        "    # import pathlib\n",
        "    # dstpath = os.path.join(gwork,'flower_photos.tar.gz')\n",
        "    # data_root_orig = tf.keras.utils.get_file(origin='https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    #                                    fname=dstpath)\n",
        "    # data_root = pathlib.Path(data_root_orig)\n",
        "    # print(data_root, type(data_root), data_root.is_dir())\n",
        "\n",
        "\n",
        "    # #Hyperparameters and statistics\n",
        "    # batch_size = 64\n",
        "    # iterations = 30000\n",
        "\n",
        "    # d_loss = []\n",
        "    # g_loss = []\n",
        "\n",
        "    # # # download pokemon.npy\n",
        "    # # !gdown https://drive.google.com/uc?id=1cmu3uuBH91E_laIC7wnCellfJv5cSa0O\n",
        "\n",
        "\n",
        "    # #Import data\n",
        "    # x_train = np.load('data.npy')\n",
        "\n",
        "\n",
        "    # #Normalize data\n",
        "    # x_train = x_train.astype('float32') / 255.0\n",
        "\n",
        "    # image_indices = np.random.randint(0, x_train.shape[0] - 1, [4])\n",
        "    # real_images = x_train[image_indices]\n",
        "\n",
        "    # for i in range(4):\n",
        "    #     plt.figure(i)\n",
        "    #     plt.imshow(real_images[i])\n",
        "\n",
        "    # plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqMyGfc9HPac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldLZCIXOC1bJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls -la \"/content/gdrive/My Drive/gwork/flower_photos\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy6ZtXMuW_9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQz4VFRDbkOl",
        "colab_type": "text"
      },
      "source": [
        "# === MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHaCuL3SlMQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MODEL MNIST\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class MNISTMODEL(tf.keras.models.Model):\n",
        "\n",
        "    def __init__(self, \n",
        "               img_rows=128, \n",
        "               img_cols=128, \n",
        "               channels=4, # \n",
        "               latent_dim=3, # !\n",
        "               loss='binary_crossentropy',\n",
        "               optimizer=tf.keras.optimizers.Adam(1e-4)\n",
        "               ):\n",
        "    \n",
        "        super(MNISTMODEL, self).__init__()\n",
        "\n",
        "        self.qepoch = tf.Variable(0.) # _e_\n",
        "\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        self.channels = channels\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.generator = self.make_generator_model()\n",
        "        self.discriminator = self.make_discriminator_model()\n",
        "\n",
        "        # will train two networks separately\n",
        "        self.generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "        self.discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "        # method returning a helper function to compute cross entropy loss\n",
        "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "        # Loss function for training:\n",
        "        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "        # Train loss\n",
        "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "        # Test loss\n",
        "        self.test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "        self.test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "        self.chkpt = {\n",
        "            'generator_optimizer': self.generator_optimizer,\n",
        "            'discriminator_optimizer': self.discriminator_optimizer,\n",
        "            'generator': self.generator,\n",
        "            'discriminator': self.discriminator,\n",
        "            'qepoch': self.qepoch,\n",
        "        }\n",
        "\n",
        "\n",
        "    # compare the discriminator's predictions on real images to an array of 1s, \n",
        "    # and the discriminator's predictions on fake (generated) images to an array of 0s\n",
        "    def discriminator_loss(self, real_output, fake_output):\n",
        "        real_loss = self.cross_entropy(tf.ones_like(real_output), real_output)\n",
        "        fake_loss = self.cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "        total_loss = real_loss + fake_loss\n",
        "        return total_loss    \n",
        "\n",
        "    # compare the discriminators decisions on the generated images to an array of 1s\n",
        "    def generator_loss(self, fake_output):\n",
        "        return self.cross_entropy(tf.ones_like(fake_output), fake_output)      \n",
        "\n",
        "    # The generator uses tf.keras.layers.Conv2DTranspose (upsampling) layers \n",
        "    def make_generator_model(self):\n",
        "\n",
        "        # # ================================ mnist\n",
        "        model = tf.keras.Sequential()\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(7*7*256, \n",
        "                                use_bias=False, \n",
        "                                input_shape=(100,)))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.LeakyReLU())\n",
        "        model.add(tf.keras.layers.Reshape((7, 7, 256)))\n",
        "        assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "        model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), \n",
        "                                        padding='same', use_bias=False))\n",
        "        assert model.output_shape == (None, 7, 7, 128)\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "        model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), \n",
        "                                        padding='same', use_bias=False))\n",
        "        assert model.output_shape == (None, 14, 14, 64)\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "        model.add(tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', \n",
        "                                        use_bias=False, activation='tanh'))\n",
        "        \n",
        "        img_rows = 28\n",
        "        img_cols = 28\n",
        "        channels = 1\n",
        "        assert model.output_shape == (None, img_rows, img_cols, channels)\n",
        "\n",
        "        return model\n",
        " \n",
        "\n",
        "\n",
        "    # The discriminator is a CNN-based image classifier.\n",
        "    def make_discriminator_model(self):\n",
        "        img_rows = 28\n",
        "        img_cols = 28\n",
        "        channels = 1\n",
        "\n",
        "        # ================================ mnist    \n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                        input_shape=[img_rows, img_cols, channels]))\n",
        "        model.add(tf.keras.layers.LeakyReLU())\n",
        "        model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "        model.add(tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "        model.add(tf.keras.layers.LeakyReLU())\n",
        "        model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "        model.add(tf.keras.layers.Flatten())\n",
        "        model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    # *************\n",
        "    #   call\n",
        "    def call(self, inputs):\n",
        "        G = self.generator\n",
        "  \n",
        "        generated_image = G(inputs, training=False)\n",
        "        return generated_image\n",
        "\n",
        "    # *************\n",
        "    #   train\n",
        "    def train(self, \n",
        "              inputs,\n",
        "              epochs_batch = 5,\n",
        "              batch_size = 256,\n",
        "              buffer_size = 60000,\n",
        "              imgtracefolder = 'imgtrace',\n",
        "              ckptfolder = 'mnistckpt', # gwork + key for ckpoints\n",
        "              noise_dim = 100, # is latent_dim\n",
        "              epochs = 50, # \n",
        "              num_examples_to_generate = 16 #\n",
        "              ):   \n",
        "      \n",
        "        model = self\n",
        "\n",
        "        self.epochs_batch = epochs_batch\n",
        "        self.batch_size = batch_size\n",
        "        self.imgtracefolder = imgtracefolder\n",
        "        self.ckptfolder = ckptfolder\n",
        "\n",
        "        self.noise_dim = noise_dim\n",
        "        self.num_examples_to_generate = num_examples_to_generate\n",
        "\n",
        "        # to visualize progress in the animated GIF)\n",
        "        self.seed = tf.random.normal([self.num_examples_to_generate, \n",
        "                                      self.noise_dim])\n",
        "\n",
        "        self.train_loop(inputs, epochs)\n",
        "\n",
        "    # ******************\n",
        "    #   train_step\n",
        "    #       \n",
        "    #   Notice the use of `tf.function`\n",
        "    #   This annotation causes the function to be \"compiled\"\n",
        "    #\n",
        "    @tf.function\n",
        "    def train_step(self, images):\n",
        "        model = self\n",
        "        batch_size = self.batch_size\n",
        "        noise_dim = self.noise_dim\n",
        "\n",
        "        noise = tf.random.normal([batch_size, noise_dim])\n",
        "\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            generated_images = model.generator(noise, training=True)\n",
        "\n",
        "            real_output = model.discriminator(images, training=True)\n",
        "            fake_output = model.discriminator(generated_images, training=True)\n",
        "\n",
        "            gen_loss = model.generator_loss(fake_output)\n",
        "            disc_loss = model.discriminator_loss(real_output, fake_output)\n",
        "\n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, \n",
        "                                                model.generator.trainable_variables)\n",
        "        gradients_of_discriminator = disc_tape.gradient(disc_loss, \n",
        "                                                        model.discriminator.trainable_variables)\n",
        "\n",
        "        model.generator_optimizer.apply_gradients(zip(gradients_of_generator, \n",
        "                                                model.generator.trainable_variables))\n",
        "        model.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, \n",
        "                                                model.discriminator.trainable_variables))    \n",
        "\n",
        "    # ******************\n",
        "    #   train_loop\n",
        "    #       \n",
        "    #\n",
        "    def train_loop(self, \n",
        "                   dataset, \n",
        "                   epochs):\n",
        "\n",
        "        model = self\n",
        "\n",
        "        ckpt_prefix = exchecker.get_ckpt_prefix()\n",
        "        ckpt = exchecker.get_checkpoint() # get model from self\n",
        "        mgr = exchecker.get_manager(ckpt)\n",
        "        ckptloadstatus = ckpt.restore(mgr.latest_checkpoint)\n",
        "\n",
        "        print(\"ckpt type %s\" %type(ckpt))\n",
        "        print(\"mgr type %s\" %type(mgr))\n",
        "        print(\"ckptloadstatus %s\" %ckptloadstatus)\n",
        "\n",
        "        if mgr.latest_checkpoint:\n",
        "            print(\"Restored from {}\".format(mgr.latest_checkpoint))\n",
        "        else:\n",
        "            print(\"Initializing from scratch.\")\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            ckpt.qepoch.assign_add(tf.cast(1, tf.float32))\n",
        "            tf.py_function(mgr.save, [], [tf.string])\n",
        "            print(\"epoch in run: %s; epoch in model: %s\" %(epoch, model.qepoch.numpy()))\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            # ------------------------------\n",
        "            for i,image_batch in enumerate(dataset):\n",
        "                startbatch = time.time()\n",
        "\n",
        "                self.train_step(image_batch) # train_step\n",
        "\n",
        "                timedeltabatch = time.time() - startbatch\n",
        "            # ------------------------------\n",
        "\n",
        "            # Produce images for the GIF as we go\n",
        "            # display.clear_output(wait=True)\n",
        "            if (epoch + 1) % self.epochs_batch == 0:\n",
        "                self.generate_and_save_images(epoch + 1,\n",
        "                                    self.seed,\n",
        "                                    self.imgtracefolder)\n",
        "\n",
        "            # Save the model every nepochs\n",
        "            if (epoch + 1) % self.epochs_batch == 0:\n",
        "                ckpt.save()\n",
        "\n",
        "            print ('Time for epoch {} is {} sec'.format(epoch + 1, \n",
        "                                                        time.time()-start))\n",
        "\n",
        "            rvnepoch = model.qepoch\n",
        "            tf.dtypes.cast(rvnepoch, tf.int32)\n",
        "\n",
        "            inepoch = rvnepoch.numpy()\n",
        "            inepoch = inepoch.astype(int)\n",
        "\n",
        "            print(\"rvnepoch type %s\" %type(inepoch))\n",
        "            print(\"rvnepoch value %s\" %inepoch)\n",
        "\n",
        "        # Generate after the final epoch\n",
        "        display.clear_output(wait=True)\n",
        "        self.generate_and_save_images(inepoch,\n",
        "                                          self.seed,\n",
        "                                          self.imgtracefolder)        \n",
        "\n",
        "    # Generate and save image after final epoch\n",
        "\n",
        "    def generate_and_save_images(self,\n",
        "                                 epoch, \n",
        "                                 test_input,\n",
        "                                 imgtracefolder):\n",
        "\n",
        "        model = self \n",
        "\n",
        "        # Notice `training` is set to False.\n",
        "        # This is so all layers run in inference mode (batchnorm).\n",
        "        predictions = model(test_input, training=False)\n",
        "\n",
        "        fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "        if (imgtracefolder):\n",
        "            path = imgtracefolder + '/' + 'image_at_epoch_{:04d}.png'.format(epoch)\n",
        "        else:\n",
        "            path = 'image_at_epoch_{:04d}.png'.format(epoch)\n",
        "\n",
        "        print(\"generate_and_save_images: show\")\n",
        "        for i in range(predictions.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.savefig(path)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "#   ***********************\n",
        "#\n",
        "#   TRAIN\n",
        "#\n",
        "\n",
        "# DATA\n",
        "from IPython import display\n",
        "\n",
        "if 1 or MODITEM == 'MINST':\n",
        "\n",
        "    dstpath='mnist.npz'\n",
        "    urd = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/'\n",
        "    urf = 'mnist.npz'\n",
        "\n",
        "    datagen = DATAGEN()\n",
        "    data = datagen.load_data(dstpath, urd, urf)\n",
        "    print(\"data type: %s, len %s\\n\" %(type(data), len(list(data))))\n",
        "\n",
        "    (train_images, train_labels), (_, _) = data\n",
        "    print(\"train_images type: %s, len %s\\n\" %(type(train_images), len(list(train_images))))\n",
        "\n",
        "    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "    train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
        "\n",
        "    # DATA\n",
        "    buffer_size = 60000\n",
        "    batch_size = 256\n",
        "\n",
        "\n",
        "    # Batch and shuffle the data\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices(train_images).shuffle(buffer_size).batch(batch_size)\n",
        "\n",
        "\n",
        "    # sliced_images =  tf.data.Dataset.from_tensor_slices(train_images)\n",
        "    # shuffled_images = sliced_images.shuffle(buffer_size)\n",
        "    # print(\"shuffled_images type: %s, len %s\\n\" %(type(shuffled_images), len(list(shuffled_images))))\n",
        "\n",
        "\n",
        "    # batch_of_images = shuffled_images.batch(batch_size) # !!!\n",
        "    # print(\"batch_of_images type: %s, len %s\\n\" %(type(batch_of_images), len(list(batch_of_images))))\n",
        "\n",
        "    # train_ds = shuffled_images # \n",
        "    print(\"train_ds type: %s, len %s\\n\" %(type(train_ds), len(list(train_ds))))\n",
        "\n",
        "# TRAIN\n",
        "if 1:\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # ds = iter(train_dataset) # batch.shape: (256, 28, 28, 1)\n",
        "    # print(\"ds type: %s\\n\" %type(ds))\n",
        "\n",
        "    # batch = next(ds) # tf.Tensor (256, 28, 28, 1)\n",
        "    # elem = batch[0]\n",
        "    # shape = elem.shape \n",
        "    # print(\"elem shape: %s\\n\" %shape)\n",
        "\n",
        "    # fig, ax = plt.subplots(figsize=(18, 2))\n",
        "    # ax.imshow(elem[:, :, 0], cmap='gray')\n",
        "\n",
        "    # MNISTMODEL\n",
        "    noise_dim = 100\n",
        "    mnistmodel = MNISTMODEL(\n",
        "        noise_dim\n",
        "        )\n",
        "\n",
        "    # EXCHECKER\n",
        "    exchecker = EXCHECKER(mnistmodel,\n",
        "                        checkpoint_dir = gwork + '/mnistckpts')    \n",
        "    checkpoint = exchecker.get_checkpoint() # <tensorflow.python.training.tracking.util.Checkpoint object at 0x7f3bf346f588>\n",
        "\n",
        "    # # PRE-TRAIN\n",
        "    G = mnistmodel.generator\n",
        "    D = mnistmodel.discriminator\n",
        "\n",
        "\n",
        "    noise = tf.random.normal([1, noise_dim])\n",
        "    generated_image = G(noise, training=False)\n",
        "\n",
        "    print (\"show generated image from noice\")\n",
        "    plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
        "    decision = D(generated_image)\n",
        "    print (\"is real: %s\\n\" %decision)\n",
        "\n",
        "\n",
        "    # TRAINER\n",
        "    tracefolder = gwork + '/mnistimgtrace'\n",
        "    if not os.path.exists(tracefolder):\n",
        "        os.mkdir(tracefolder)\n",
        "\n",
        "    mnistmodel.train(train_ds,\n",
        "                     epochs = 50, # will run epochs\n",
        "                     epochs_batch = 5, # checkout by\n",
        "                     batch_size = 256, # with batch of\n",
        "                     imgtracefolder = tracefolder)\n",
        "\n",
        "\n",
        "    # ANIME\n",
        "\n",
        "    anime = ANIME(model)\n",
        "\n",
        "    gif = anime.generate_gif(srcfolder=gwork + '/mnistimgtrace', \n",
        "                             dstfile = 'ani.gif')         \n",
        "    anime.display_gif(giffile='ani.gif')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa2mrIOffNWY",
        "colab_type": "text"
      },
      "source": [
        "# === STYLEBASE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiJxVnEWSHo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#   STYLEBASE\n",
        "#\n",
        "class STYLEBASE(tf.keras.models.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 image,\n",
        "                 verbose = True,\n",
        "                 ):\n",
        "\n",
        "        super(STYLEBASE, self).__init__()\n",
        "\n",
        "        self.image = image\n",
        "\n",
        "\n",
        "    def predict(self):\n",
        "        #\n",
        "        #   Load a VGG19 with the classification head (include_top=True)\n",
        "        #   and test run it on the content image to ensure it's used correctly\n",
        "        #\n",
        "        x = tf.keras.applications.vgg19.preprocess_input(self.image*255)\n",
        "        x = tf.image.resize(x, (224, 224))\n",
        "        vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
        "\n",
        "        prediction_probabilities = vgg(x)\n",
        "        prediction_probabilities.shape\n",
        "\n",
        "        predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
        "        return [(class_name, prob) for (number, class_name, prob) in predicted_top_5]\n",
        "\n",
        "#   ***********************\n",
        "#\n",
        "#   TRAIN\n",
        "#\n",
        "\n",
        "if 0:\n",
        "    # DATA STYLER\n",
        "\n",
        "    from shutil import copyfile\n",
        "\n",
        "    Datagen = DATAGEN()\n",
        "    #\n",
        "    # to style - nasa\n",
        "    #\n",
        "    tostylefolder = \"nasa/\"\n",
        "    gtostylefolder = gdata + tostylefolder\n",
        "    print(\"gtostylefolder: %s\" %gtostylefolder)\n",
        "\n",
        "    URD_A = 'https://www.nasa.gov/sites/default/files/styles/full_width_feature/public/thumbnails/image/'\n",
        "    URFS_A = [\n",
        "            \"potw1934a.jpg\", \n",
        "            ]\n",
        "    DIR_A = tostylefolder\n",
        "\n",
        "\n",
        "    Datagen.get_rawdata(DIR_A, URD_A, URFS_A )\n",
        "\n",
        "    # if folder of images to style does not e xist\n",
        "    if not os.path.exists(tostylefolder):\n",
        "        os.mkdir(tostylefolder)\n",
        "\n",
        "    # if gdata folder is available\n",
        "    if os.path.exists(gtostylefolder):\n",
        "        Datagen.get_gdata(gtostylefolder, tostylefolder)\n",
        "\n",
        "    # if gdata folder not available, download data\n",
        "    else:\n",
        "        Datagen.get_rawdata(DIR_A, URD_A, URFS_A)\n",
        "\n",
        "\n",
        "\n",
        "    styledfolder = \"cortes/\"\n",
        "    gstyledfolder = gdata + styledfolder\n",
        "    print(\"gstyledfolder: %s\" %gstyledfolder)\n",
        "\n",
        "    URD_B = 'https://rehs.com/catalogimages/'\n",
        "    URFS_B = [\n",
        "            'edouard_cortes_e1339_pont_neuf_wm.jpg',\n",
        "            ]\n",
        "    DIR_B = styledfolder\n",
        "\n",
        "    # if folder of images to style does not e xist\n",
        "    if not os.path.exists(styledfolder):\n",
        "        os.mkdir(styledfolder)\n",
        "\n",
        "    # if gdata folder is available\n",
        "    if os.path.exists(gstyledfolder):\n",
        "        Datagen.get_gdata(gstyledfolder, styledfolder)\n",
        "\n",
        "    # if gdata folder not available, download data\n",
        "    else:\n",
        "        Datagen.get_rawdata(DIR_B, URD_B, URFS_B) # tf.keras.utils.get_file(file, url)\n",
        "\n",
        "\n",
        "    # SINGLE IMAGEs\n",
        "\n",
        "    Imagine = IMAGINE()\n",
        "\n",
        "    content_path = os.path.join(DIR_A, URFS_A[0])\n",
        "    content_image = Imagine.load_img(content_path)\n",
        "\n",
        "    style_path = os.path.join(DIR_B, URFS_B[0])\n",
        "    style_image = Imagine.load_img(style_path)\n",
        "\n",
        "\n",
        "    # content_image\n",
        "    stylebase = STYLEBASE(content_image)\n",
        "    prediction = stylebase.predict()\n",
        "    print(prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PA8pTnfRpZj",
        "colab_type": "text"
      },
      "source": [
        "# === STYLEHUB\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MLisM0nX8hXt",
        "colab": {}
      },
      "source": [
        "#\n",
        "#   Fast Style Transfer using TF-Hub\n",
        "#\n",
        "#\n",
        "#   STYLEHUB\n",
        "#\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "class STYLEHUB():\n",
        "\n",
        "    def __init__(self,\n",
        "                 style_image,\n",
        "                 content_image,\n",
        "                 verbose = True,\n",
        "                 ):\n",
        "        \n",
        "        super(STYLEHUB, self).__init__()\n",
        "\n",
        "        self.style_image = style_image\n",
        "        self.content_image = content_image\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def predict(self):\n",
        "        content_image = self.content_image\n",
        "        style_image = self.style_image\n",
        "\n",
        "        hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/1')\n",
        "        stylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]\n",
        "\n",
        "        return stylized_image\n",
        "\n",
        "\n",
        "#   ******************\n",
        "#\n",
        "#   DATA\n",
        "#\n",
        "if 0:\n",
        "    from shutil import copyfile\n",
        "\n",
        "    Datagen = DATAGEN()\n",
        "    #\n",
        "    # to style - nasa\n",
        "    #\n",
        "    tostylefolder = \"nasa/\"\n",
        "    gtostylefolder = gdata + tostylefolder\n",
        "    print(\"gtostylefolder: %s\" %gtostylefolder)\n",
        "\n",
        "    URD_A = 'https://www.nasa.gov/sites/default/files/styles/full_width_feature/public/thumbnails/image/'\n",
        "    URFS_A = [\n",
        "            \"potw1934a.jpg\", \n",
        "            ]\n",
        "    DIR_A = tostylefolder\n",
        "\n",
        "\n",
        "    Datagen.get_rawdata(DIR_A, URD_A, URFS_A )\n",
        "\n",
        "    # if folder of images to style does not e xist\n",
        "    if not os.path.exists(tostylefolder):\n",
        "        os.mkdir(tostylefolder)\n",
        "\n",
        "    # if gdata folder is available\n",
        "    if os.path.exists(gtostylefolder):\n",
        "        Datagen.get_gdata(gtostylefolder, tostylefolder)\n",
        "\n",
        "    # if gdata folder not available, download data\n",
        "    else:\n",
        "        Datagen.get_rawdata(DIR_A, URD_A, URFS_A)\n",
        "\n",
        "\n",
        "\n",
        "    styledfolder = \"cortes/\"\n",
        "    gstyledfolder = gdata + styledfolder\n",
        "    print(\"gstyledfolder: %s\" %gstyledfolder)\n",
        "\n",
        "    URD_B = 'https://rehs.com/catalogimages/'\n",
        "    URFS_B = [\n",
        "            'edouard_cortes_e1339_pont_neuf_wm.jpg',\n",
        "            ]\n",
        "    DIR_B = styledfolder\n",
        "\n",
        "    # if folder of images to style does not e xist\n",
        "    if not os.path.exists(styledfolder):\n",
        "        os.mkdir(styledfolder)\n",
        "\n",
        "    # if gdata folder is available\n",
        "    if os.path.exists(gstyledfolder):\n",
        "        Datagen.get_gdata(gstyledfolder, styledfolder)\n",
        "\n",
        "    # if gdata folder not available, download data\n",
        "    else:\n",
        "        Datagen.get_rawdata(DIR_B, URD_B, URFS_B) # tf.keras.utils.get_file(file, url)\n",
        "\n",
        "\n",
        "    # SINGLE IMAGEs\n",
        "    content_path = os.path.join(DIR_A, URFS_A[0])\n",
        "    content_image = Imagine.load_img(content_path)\n",
        "\n",
        "    style_path = os.path.join(DIR_B, URFS_B[0])\n",
        "    style_image = Imagine.load_img(style_path)\n",
        "\n",
        "\n",
        "    # EVAL\n",
        "\n",
        "    stylehub = STYLEHUB(style_image, content_image)       \n",
        "    image = stylehub.predict()\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    Imagine.imshow(image, 'image')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYwituIQ2Ht6",
        "colab_type": "text"
      },
      "source": [
        "# === MODELVGG\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zjOMc3yM9Fyv",
        "colab": {}
      },
      "source": [
        "#   MODELVGG\n",
        "#\n",
        "class MODELVGG(tf.keras.models.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 image,\n",
        "                 verbose = True,\n",
        "                 ):\n",
        "\n",
        "        super(MODELVGG, self).__init__()\n",
        "        self.image = image\n",
        "\n",
        "    def predict(self):\n",
        "        #\n",
        "        #   Load a VGG19 with the classification head (include_top=True)\n",
        "        #   and test run it on the content image to ensure it's used correctly\n",
        "        #\n",
        "        x = tf.keras.applications.vgg19.preprocess_input(self.image*255)\n",
        "        x = tf.image.resize(x, (224, 224))\n",
        "        vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
        "\n",
        "        prediction_probabilities = vgg(x)\n",
        "        prediction_probabilities.shape\n",
        "\n",
        "        predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
        "        return [(class_name, prob) for (number, class_name, prob) in predicted_top_5]\n",
        "\n",
        "#   ***********************\n",
        "#\n",
        "#   EVAL\n",
        "#\n",
        "\n",
        "if 0:\n",
        "    # DATA STYLER\n",
        "\n",
        "    from shutil import copyfile\n",
        "\n",
        "    datagen = DATAGEN()\n",
        "\n",
        "    #\n",
        "    # to style - nasa\n",
        "    #\n",
        "    gdir_a = gdata + '/nasa'\n",
        "    wdir_a = gwork + '/nasa'\n",
        "    print(\"gdir_a: %s\" %gdir_a)\n",
        "    print(\"wdir_a: %s\" %wdir_a)\n",
        "\n",
        "    URD_A = 'https://www.nasa.gov/sites/default/files/styles/full_width_feature/public/thumbnails/image/'\n",
        "    URFS_A = [\n",
        "            \"potw1934a.jpg\", \n",
        "            ]\n",
        "\n",
        "    datagen.get_uriitems(URD_A, URFS_A, gdir_a, wdir_a)\n",
        "\n",
        "    # \n",
        "    #   styled =cortes\n",
        "    #\n",
        "    gdir_b = gdata + '/cortes'\n",
        "    wdir_b = gwork + '/cortes'\n",
        "    print(\"gdir_b: %s\" %gdir_b)\n",
        "    print(\"wdir_b: %s\" %wdir_b)\n",
        "\n",
        "    URD_B = 'https://rehs.com/catalogimages/'\n",
        "    URFS_B = [\n",
        "            'edouard_cortes_e1339_pont_neuf_wm.jpg',\n",
        "            ]\n",
        "\n",
        "    datagen.get_uriitems(URD_B, URFS_B, gdir_b, wdir_b)\n",
        "\n",
        "\n",
        "    # SINGLE IMAGEs\n",
        "\n",
        "    Imagine = IMAGINE()\n",
        "\n",
        "    content_path = os.path.join(wdir_a, URFS_A[0])\n",
        "    print(\"content_path: %s\" %content_path)\n",
        "    content_image = Imagine.load_img(content_path)\n",
        "\n",
        "    style_path = os.path.join(wdir_b, URFS_B[0])\n",
        "    print(\"style_path: %s\" %style_path)\n",
        "    style_image = Imagine.load_img(style_path)\n",
        "\n",
        "    # content_image\n",
        "    print(\"content_image type: %s\" %type(content_image))\n",
        "    modelvgg = MODELVGG(content_image)\n",
        "    prediction = modelvgg.predict()\n",
        "    print(prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5QALG35CnW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge-GTucrCfGE",
        "colab_type": "text"
      },
      "source": [
        "# === STYLER\n",
        "\n",
        "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/style_transfer.ipynb\n",
        "\n",
        "https://github.com/manicman1999/Style-Transfer-TF2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JFWEm882-DQd",
        "colab": {}
      },
      "source": [
        "#   ***********************\n",
        "#\n",
        "#   STYLER\n",
        "#\n",
        "#   Build a model that returns the style and content tensors\n",
        "#\n",
        "import IPython.display as display\n",
        "\n",
        "class STYLERMOD(tf.keras.models.Model):\n",
        "\n",
        "    def __init__(self, \n",
        "                 style_image = None,\n",
        "                 content_image = None,\n",
        "                 verbose = False,\n",
        "                 total_variation = True,\n",
        "                 ):\n",
        "\n",
        "        super(STYLERMOD, self).__init__()\n",
        "\n",
        "        self.total_variation=total_variation\n",
        "\n",
        "\n",
        "        # weight for the total_variation_loss\n",
        "        self.total_variation_weight=30\n",
        "\n",
        "        self.style_image = style_image\n",
        "        self.content_image = content_image\n",
        "\n",
        "        # Choose intermediate layers from the network to represent the style and content of the image\n",
        "        # VGG19 Content layer - encoder\n",
        "        self.content_layers = ['block5_conv2'] \n",
        "\n",
        "        # VGG19 Style layer\n",
        "        self.style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1', \n",
        "                'block4_conv1', \n",
        "                'block5_conv1']\n",
        "\n",
        "        self.num_style_layers = len(self.style_layers)\n",
        "        self.num_content_layers = len(self.content_layers)\n",
        "\n",
        "        # create an optimizer\n",
        "        self.opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
        "\n",
        "        #\n",
        "        #   model\n",
        "        #\n",
        "        self.vgg = self.vgg_layers(self.style_layers + self.content_layers)\n",
        "        self.vgg.trainable = False\n",
        "\n",
        "        if verbose:\n",
        "            for layer in self.vgg.layers:\n",
        "                print(layer.name)\n",
        "\n",
        "        self.style_extractor = self.vgg_layers(self.style_layers)\n",
        "        self.style_outputs = self.style_extractor(style_image*255)\n",
        "        #Look at the statistics of each layer's output\n",
        "        if verbose:\n",
        "            for name, output in zip(self.style_layers, self.style_outputs):\n",
        "                print(name)\n",
        "                print(\"  shape: \", output.numpy().shape)\n",
        "                print(\"  min: \", output.numpy().min())\n",
        "                print(\"  max: \", output.numpy().max())\n",
        "                print(\"  mean: \", output.numpy().mean())\n",
        "                print()\n",
        "\n",
        "\n",
        "        #\n",
        "        #   chkpt\n",
        "        #\n",
        "        self.epoch = tf.Variable(0.) # _e_        \n",
        "        self.chkpt = {\n",
        "            'vgg': self.vgg,\n",
        "            'epoch': self.epoch,\n",
        "        }\n",
        "\n",
        "    \n",
        "\n",
        "        #\n",
        "        #   called on an image, the model returns the gram matrix (style) \n",
        "        #   of the style_layers and content of the content_layers\n",
        "        #\n",
        "        self.results = self(tf.constant(self.content_image)) # content_image\n",
        "        self.style_results = self.results['style']\n",
        "\n",
        "        if verbose:\n",
        "            print('Styles:')\n",
        "            for name, output in sorted(self.results['style'].items()):\n",
        "                print(\"  \", name)\n",
        "                print(\"    shape: \", output.numpy().shape)\n",
        "                print(\"    min: \", output.numpy().min())\n",
        "                print(\"    max: \", output.numpy().max())\n",
        "                print(\"    mean: \", output.numpy().mean())\n",
        "                print()\n",
        "\n",
        "            print(\"Contents:\")\n",
        "            for name, output in sorted(self.results['content'].items()):\n",
        "                print(\"  \", name)\n",
        "                print(\"    shape: \", output.numpy().shape)\n",
        "                print(\"    min: \", output.numpy().min())\n",
        "                print(\"    max: \", output.numpy().max())\n",
        "                print(\"    mean: \", output.numpy().mean())\n",
        "\n",
        "\n",
        "    #   *******************\n",
        "    #   vgg_layers\n",
        "    #\n",
        "    def vgg_layers(self, layer_names):\n",
        "\n",
        "        # load a VGG19 without the classification head\n",
        "\n",
        "        vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "        vgg.trainable = False\n",
        "\n",
        "        print()\n",
        "        for layer in vgg.layers:\n",
        "           continue # print(layer.name)\n",
        "\n",
        "        \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
        "        # Load our model. Load pretrained VGG, trained on imagenet data\n",
        "        \n",
        "        inputs = [vgg.input]\n",
        "        outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "        model = tf.keras.Model(inputs, outputs)\n",
        "        return model\n",
        "        \n",
        "    #   ***************\n",
        "    #   style_content_loss\n",
        "    #\n",
        "    def style_content_loss(self, outputs):\n",
        "\n",
        "        style_image = self.style_image\n",
        "        content_image = self.content_image\n",
        "\n",
        "        # weighted combination of the two losses to get the total loss\n",
        "        style_weight=1e-2\n",
        "        content_weight=1e4\n",
        "\n",
        "        # set style and content target values\n",
        "        style_targets = self(style_image)['style']\n",
        "        content_targets = self(content_image)['content']\n",
        "\n",
        "        style_outputs = outputs['style']\n",
        "        content_outputs = outputs['content']\n",
        "        style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n",
        "                            for name in style_outputs.keys()])\n",
        "        style_loss *= style_weight / self.num_style_layers\n",
        "\n",
        "        content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n",
        "                                for name in content_outputs.keys()])\n",
        "        content_loss *= content_weight / self.num_content_layers\n",
        "        loss = style_loss + content_loss\n",
        "        return loss\n",
        "\n",
        "    # *************\n",
        "    #   gram_matrix\n",
        "    #   The content of an image is represented by the values of the intermediate feature maps\n",
        "    #   the style of an image can be described by the means and correlations across the different feature map\n",
        "    #\n",
        "    #   Calculate a Gram matrix that includes the style by taking the outer product \n",
        "    #   of the feature vector with itself at each location, and averaging that outer \n",
        "    #   product over all locations \n",
        "    #\n",
        "    def gram_matrix(self, input_tensor):\n",
        "        result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "        input_shape = tf.shape(input_tensor)\n",
        "        num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "        return result/(num_locations)\n",
        "\n",
        "    # *************\n",
        "    #   call\n",
        "    #   When called on an image, this model returns the gram matrix (style) \n",
        "    #   of the style_layers and content of the content_layers\n",
        "    #\n",
        "    #   return the style and content tensors\n",
        "    #\n",
        "    def call(self, inputs):\n",
        "        \"Expects float input in [0,1]\"\n",
        "        inputs = inputs*255.0\n",
        "        \n",
        "        # preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
        "        preprocessed_input = inputs\n",
        "\n",
        "        outputs = self.vgg(preprocessed_input)\n",
        "        style_outputs, content_outputs = (outputs[:self.num_style_layers], \n",
        "                                        outputs[self.num_style_layers:])\n",
        "\n",
        "        style_outputs = [self.gram_matrix(style_output)\n",
        "                        for style_output in style_outputs]\n",
        "\n",
        "        content_dict = {content_name:value \n",
        "                        for content_name, value \n",
        "                        in zip(self.content_layers, content_outputs)}\n",
        "\n",
        "        style_dict = {style_name:value\n",
        "                    for style_name, value\n",
        "                    in zip(self.style_layers, style_outputs)}\n",
        "        \n",
        "        return {'content':content_dict, 'style':style_dict}\n",
        "\n",
        "\n",
        "    # *************\n",
        "    #   train\n",
        "    #\n",
        "    def train(self, \n",
        "              inputs,\n",
        "                 outimgfolder = 'outimgfolder',\n",
        "                 imgtracefolder = 'imgtrace',\n",
        "                 ckptfolder = 'mnistckpt',\n",
        "                 batch_size = 128,\n",
        "                 epochs = 50,\n",
        "                 epochs_batch = 10,\n",
        "                 noise_dim = 3,\n",
        "                 save_interval = 100,\n",
        "                 latent_dim = 128,\n",
        "                 total_variation = True,\n",
        "                 num_examples_to_generate = 16,\n",
        "                 steps_per_epoch = 100\n",
        "              ):  \n",
        "    \n",
        "        model = self\n",
        "\n",
        "        self.train_loop(inputs, \n",
        "                        epochs, \n",
        "                        steps_per_epoch, \n",
        "                        epochs_batch,\n",
        "                        imgtracefolder,\n",
        "                        total_variation,\n",
        "                        )\n",
        "\n",
        "    #   **********************\n",
        "    #   total_variation_loss\n",
        "    #\n",
        "    # Use tf.GradientTape to update the image\n",
        "    def total_variation_loss(self, image):\n",
        "        x_deltas, y_deltas = high_pass_x_y(image)\n",
        "        return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))\n",
        "\n",
        "    #   **********************\n",
        "    #   train_step\n",
        "    #\n",
        "    @tf.function()\n",
        "    def train_step(self, image, total_variation = True):\n",
        "        \n",
        "        model = self\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = model(image)\n",
        "            loss = model.style_content_loss(outputs)\n",
        "\n",
        "            if total_variation:\n",
        "                loss += self.total_variation_weight*tf.image.total_variation(image)\n",
        "\n",
        "        grad = tape.gradient(loss, image)\n",
        "        model.opt.apply_gradients([(grad, image)])\n",
        "        image.assign(Imagine.clip_0_1(image))\n",
        "\n",
        "    #   **********************\n",
        "    #   train_loop\n",
        "    #\n",
        "    def train_loop(self, \n",
        "                   inputs, \n",
        "                   epochs = 10, \n",
        "                   epochs_batch = 10,\n",
        "                   steps_per_epoch = 80,\n",
        "                   imgtracefolder = \"imgtrace\",\n",
        "                   total_vatiation = True):\n",
        "      \n",
        "        model = self\n",
        "        image = inputs\n",
        "\n",
        "        print(\"train_loop epochs: %s , steps: %s \" %(epochs, steps_per_epoch))\n",
        "\n",
        "        ckpt_prefix = exchecker.get_ckpt_prefix()\n",
        "        ckpt = exchecker.get_checkpoint() # get model from self\n",
        "        mgr = exchecker.get_manager(ckpt)\n",
        "        ckptloadstatus = ckpt.restore(mgr.latest_checkpoint)\n",
        "\n",
        "        print(\"ckpt type %s\" %type(ckpt))\n",
        "        print(\"mgr type %s\" %type(mgr))\n",
        "        print(\"ckptloadstatus %s\" %ckptloadstatus)\n",
        "\n",
        "        if mgr.latest_checkpoint:\n",
        "            print(\"Restored from {}\".format(mgr.latest_checkpoint))\n",
        "        else:\n",
        "            print(\"Initializing from scratch.\")\n",
        "\n",
        "\n",
        "        step = 0\n",
        "        for epoch in range(epochs):\n",
        "            ckpt.epoch.assign_add(tf.cast(1, tf.float32))\n",
        "            tf.py_function(mgr.save, [], [tf.string])\n",
        "            print(\"epoch in run: %s; epoch in model: %s\" %(epoch, model.epoch.numpy()))\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            # ------------------------------\n",
        "            for m in range(steps_per_epoch):\n",
        "                step += 1\n",
        "                tim = time.time()\n",
        "                # if (epoch + 1) % epochs_batch == 0:\n",
        "                #     print('train epoch %s step %s at %s' %(epoch, m, tim))\n",
        "                if 0:   # would display image\n",
        "                    display.display(Imagine.tensor_to_image(image))\n",
        "\n",
        "                # --------------------------\n",
        "\n",
        "                self.train_step(image, total_variation = True)\n",
        "\n",
        "                # --------------------------\n",
        "                print(\".\", end='')\n",
        "            # ------------------------------        \n",
        "\n",
        "            # Save the model every nepochs\n",
        "            \n",
        "            if (epoch + 1) % epochs_batch == 0:\n",
        "                ckpt.save()\n",
        "\n",
        "            print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "            rvnepoch = model.epoch\n",
        "            tf.dtypes.cast(rvnepoch, tf.int32)\n",
        "\n",
        "            inepoch = rvnepoch.numpy()\n",
        "            inepoch = inepoch.astype(int)\n",
        "\n",
        "            print(\"rvnepoch %s %inepoch type %s\" %type(inepoch))\n",
        "            \n",
        "            if (epoch + 1) % epochs_batch == 0:\n",
        "                self.generate_and_save_images(image, inepoch + 1, imgtracefolder)\n",
        "\n",
        "        # Generate after the final epoch\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "        self.generate_and_save_images(image, epoch + 1, imgtracefolder)\n",
        "\n",
        "    # Generate and save images\n",
        "\n",
        "    def generate_and_save_images(self, \n",
        "                                 image, \n",
        "                                 epoch,\n",
        "                                 imgtracefolder):\n",
        "        \n",
        "        img = Imagine.tensor_to_image(image)\n",
        "\n",
        "        if (imgtracefolder):\n",
        "            if not os.path.exists(imgtracefolder):        \n",
        "                os.mkdir(imgtracefolder)              \n",
        "            path = imgtracefolder + '/' + 'image_at_epoch_{:04d}.png'.format(epoch)\n",
        "        else:\n",
        "            path = 'image_at_epoch_{:04d}.png'.format(epoch)\n",
        "\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "        plt.savefig(path)\n",
        "        # display.display(img)\n",
        "        plt.show()\n",
        "\n",
        "#   ************************\n",
        "#\n",
        "#   DO\n",
        "#\n",
        "if 1:\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    from PIL import Image\n",
        "\n",
        "    from shutil import copyfile\n",
        "\n",
        "    Imagine = IMAGINE()\n",
        "\n",
        "    datagen = DATAGEN()\n",
        "\n",
        "    #\n",
        "    # to style - nasa\n",
        "    #\n",
        "    gdir_a = gdata + '/nasa'\n",
        "    wdir_a = gwork + '/nasa'\n",
        "    # print(\"gdir_a: %s\" %gdir_a)\n",
        "    # print(\"wdir_a: %s\" %wdir_a)\n",
        "\n",
        "    URD_A = 'https://www.nasa.gov/sites/default/files/styles/full_width_feature/public/thumbnails/image/'\n",
        "    URFS_A = [\n",
        "            \"potw1934a.jpg\", \n",
        "            \"potw1935a.jpg\", \n",
        "            \"potw1936a.jpg\",\n",
        "            \"potw1937a.jpg\",\n",
        "            \"potw1938a.jpg\",\n",
        "            \"potw1939a.jpg\",\n",
        "            \"potw1940a.jpg\",\n",
        "            \"potw1941a.jpg\",\n",
        "            \"potw1942a.jpg\",\n",
        "            \"potw1943a.jpg\",\n",
        "            ]\n",
        "    datagen.get_uriitems(URD_A, URFS_A, gdir_a, wdir_a)\n",
        "\n",
        "\n",
        "    # \n",
        "    #   styled =cortes\n",
        "    #\n",
        "    gdir_b = gdata + '/cortes'\n",
        "    wdir_b = gwork + '/cortes'\n",
        "    # print(\"gdir_b: %s\" %gdir_b)\n",
        "    # print(\"wdir_b: %s\" %wdir_b)\n",
        "\n",
        "    URD_B = 'https://rehs.com/catalogimages/'\n",
        "    URFS_B = [\n",
        "            'edouard_cortes_e1339_pont_neuf_wm.jpg',\n",
        "            'edouard_leon_cortes_e1312_bouquinistes_wm.jpg',\n",
        "            'edouard_leon_cortes_e1114_moulin_de_la_galette_soir_de_neige_wm.jpg',\n",
        "            'edouard_leon_cortes_e1148_boulevard_des_allies_church_of_saint_pierre_caen_wm.jpg',\n",
        "            ]\n",
        "    datagen.get_uriitems(URD_B, URFS_B, gdir_b, wdir_b)\n",
        "\n",
        "    #   ******************\n",
        "    #   single images\n",
        "    #\n",
        "    content_path = os.path.join(wdir_a, URFS_A[0])\n",
        "    content_image = Imagine.load_img(content_path)\n",
        "\n",
        "    style_path = os.path.join(wdir_b, URFS_B[0])\n",
        "    style_image = Imagine.load_img(style_path)\n",
        "\n",
        "\n",
        "    #\n",
        "    #   DATASETs\n",
        "    #\n",
        "    XSIZE=128\n",
        "    YSIZE=128\n",
        "    NSLICES=10\n",
        "\n",
        "    BUFFER_SIZE = 6000\n",
        "    BATCH_SIZE = 256\n",
        "    CHANNELS = 3\n",
        "\n",
        "    RESIZE=0.75\n",
        "    DIRECTORY='nasa/' # 'FluidArt/'\n",
        "    NAME= 'nasa_256_128' # name='fluid_256_128'\n",
        "\n",
        "    train_dataset_a = datagen.get_dataset(\n",
        "                                    path= gwork + '/cortes/',\n",
        "                                    batch_size = BATCH_SIZE,\n",
        "                                    buffer_size = BUFFER_SIZE,\n",
        "                                    xSize=XSIZE,\n",
        "                                    ySize=YSIZE,\n",
        "                                    nSlices=NSLICES,\n",
        "                                    resize=RESIZE,\n",
        "                                    channels=CHANNELS\n",
        "                                    )\n",
        "\n",
        "    typeds = type(train_dataset_a) # <class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'> \n",
        "    # print(\"train_dataset_a type: %s\" %(type(train_dataset_a)))\n",
        "\n",
        "    it1_a = iter(train_dataset_a) # batch.shape: (256, 28, 28, 1)\n",
        "    batch_a = next(it1_a)\n",
        "    images_a = np.array(batch_a)\n",
        "    x_train_a = images_a\n",
        "\n",
        "    # assert( x_train_a.shape == (nimages * NSLICES, XSIZE, YSIZE, CHANNELS))\n",
        "    assert(isinstance(x_train_a, np.ndarray))\n",
        "\n",
        "    if 0:\n",
        "        fig, axs = plt.subplots(MOSAIC_ROWS, MOSAIC_COLS)\n",
        "        for i in range(MOSAIC_ROWS):\n",
        "            for j in range(MOSAIC_COLS):\n",
        "                axs[i,j].imshow( x_train_a[ np.random.randint(x_train_a.shape[0]) ] )\n",
        "                axs[i,j].axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    train_dataset_b = datagen.get_dataset(\n",
        "                                    path= gwork + '/nasa/',\n",
        "                                    batch_size = BATCH_SIZE,\n",
        "                                    buffer_size = BUFFER_SIZE,\n",
        "                                    xSize=XSIZE,\n",
        "                                    ySize=YSIZE,\n",
        "                                    nSlices=NSLICES,\n",
        "                                    resize=RESIZE,\n",
        "                                    channels=CHANNELS)\n",
        "\n",
        "    typeds = type(train_dataset_b) # <class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'> \n",
        "    # print(\"train_dataset_b type: %s\" %(type(train_dataset_b)))\n",
        "\n",
        "    it1_b = iter(train_dataset_b) # batch.shape: (256, 28, 28, 1)\n",
        "    batch_b = next(it1_b)\n",
        "    images_b = np.array(batch_b)\n",
        "    x_train_b = images_b\n",
        "\n",
        "    # assert( x_train_b.shape == (nimages * NSLICES, XSIZE, YSIZE, CHANNELS))\n",
        "    assert(isinstance(x_train_b, np.ndarray))\n",
        "\n",
        "    #   *****************\n",
        "    #   SHOW\n",
        "    #\n",
        "    if 0:\n",
        "        MOSAIC_ROWS = 6\n",
        "        MOSAIC_COLS = 6    \n",
        "        fig, axs = plt.subplots(MOSAIC_ROWS, MOSAIC_COLS)\n",
        "        for i in range(MOSAIC_ROWS):\n",
        "            for j in range(MOSAIC_COLS):\n",
        "                axs[i,j].imshow( x_train_b[ np.random.randint(x_train_b.shape[0]) ] )\n",
        "                axs[i,j].axis('off')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    #   *****************\n",
        "    #   TRAIN\n",
        "    #\n",
        "    content_inputs = tf.Variable(content_image)\n",
        "    print(\"style_image to model, content_image to train_step\")\n",
        "\n",
        "    stylermod = STYLERMOD(style_image = style_image, content_image = content_image,)\n",
        "    \n",
        "    exchecker = EXCHECKER(stylermod, \n",
        "                          checkpoint_dir = gwork + '/stylerckpts', clear = False)\n",
        "    \n",
        "    stylermod.train_step(content_inputs)\n",
        "\n",
        "    stylermod.train_loop(content_inputs, \n",
        "                    epochs = 10, \n",
        "                    epochs_batch = 1, # checkout by\n",
        "                    steps_per_epoch = 100,\n",
        "                    imgtracefolder = gwork + '/stylertrace')\n",
        "\n",
        "    display.display(Imagine.tensor_to_image(content_inputs))\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQQtI9_Kt8pv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ANIME\n",
        "\n",
        "anime = ANIME(stylermod)\n",
        "gif = anime.generate_gif(srcfolder=gwork + '/stylertrace', dstfile = 'ani.gif')\n",
        "anime.display_gif(giffile='ani.gif')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPYgPqi4_KD8",
        "colab_type": "text"
      },
      "source": [
        "# === ART2GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "03fV_16u_HEp",
        "colab": {}
      },
      "source": [
        "## MODEL ART2GAN\n",
        "\n",
        "from IPython import display\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class ART2GAN(Model):\n",
        "    def __init__(self,\n",
        "               img_rows=128, \n",
        "               img_cols=128, \n",
        "               channels=3, # RGB\n",
        "               latent_dim=256,\n",
        "               imgpath = 'imgpath', # img holder \n",
        "               ckptpath  = 'ckptpath', # checkpoints place\n",
        "               tracepath  = 'tracepath', # in loop folder\n",
        "               ):\n",
        "    \n",
        "        super(ART2GAN, self).__init__()\n",
        "\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        self.channels = channels\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = latent_dim\n",
        "        print(\"ART2GAN latent_dim: %s\" %latent_dim)\n",
        "\n",
        "        self.imgpath = imgpath\n",
        "        self.ckptpath = ckptpath\n",
        "        self.tracepath = tracepath\n",
        "\n",
        "        self.loss = 'binary_crossentropy'\n",
        "        self.optimizer = Adam(0.0005, 0.6)\n",
        "        self.generator_optimizer = self.optimizer\n",
        "        self.discriminator_optimizer = self.optimizer\n",
        "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)  \n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.generator = self.build_generator()\n",
        "        self.dcgan_optimizer = self.optimizer\n",
        "        self.build_dcgan()\n",
        "\n",
        "        self.qepoch = tf.Variable(0.)\n",
        "        qepoch = self.qepoch.numpy() + 0\n",
        "        print(\"************** qepoch : %s\" %qepoch )\n",
        "\n",
        "        self.chkpt = {\n",
        "            'generator': self.generator,\n",
        "            'generator_optimizer': self.generator_optimizer,\n",
        "            'discriminator': self.discriminator,\n",
        "            'discriminator_optimizer': self.discriminator_optimizer,\n",
        "            'dcgan': self.dcgan,\n",
        "            'dcgan_optimizer': self.dcgan_optimizer,\n",
        "            'qepoch': self.qepoch,\n",
        "        }\n",
        "\n",
        "    # ================================ dcgan    \n",
        "\n",
        "    def build_dcgan(self):\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the dcgan model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The dcgan model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.dcgan = Model(z, valid)\n",
        "        self.dcgan.compile(loss=self.loss, optimizer=self.dcgan_optimizer)    \n",
        "    \n",
        "    def load_weights(self, \n",
        "                     generator_file=None, \n",
        "                     discriminator_file=None):\n",
        "        print(\"DCGAN.load_weights %s %s\" %(generator_file, discriminator_file))\n",
        "        if generator_file:\n",
        "            print(\"load generator weights %s\" %s)\n",
        "            generator = self.build_generator()\n",
        "            generator.load_weights(generator_file)\n",
        "            self.generator = generator\n",
        "        else:\n",
        "            print(\"generator weights not found: %s\" %s)\n",
        "    \n",
        "        if discriminator_file:\n",
        "            print(\"load discriminator_file weights %s\" %s)\n",
        "            discriminator = self.build_discriminator()\n",
        "            discriminator.load_weights(discriminator_file)\n",
        "            self.discriminator = discriminator\n",
        "        else:\n",
        "            print(\"discriminator_file weights not found: %s\" %s)\n",
        "\n",
        "        if generator_file or discriminator_file: \n",
        "            self.build_dcgan() \n",
        "            \n",
        "    # ================================ generator\n",
        "\n",
        "    # The generator uses tf.keras.layers.Conv2DTranspose (upsampling) layers \n",
        "    def build_generator(self):\n",
        "\n",
        "        model = tf.keras.Sequential()\n",
        "        \n",
        "        #model.add(Dense(128, activation=\"relu\", input_dim=self.latent_dim, name=\"generator_input\") )\n",
        "        #model.add(Dropout(0.1))\n",
        "                \n",
        "        model.add(layers.Dense(64 * 32 * 32, activation=\"relu\",\n",
        "                               input_dim=self.latent_dim, \n",
        "                               name=\"generator_input\") )\n",
        "        model.add(tf.keras.layers.Dropout(0.3))\n",
        "        model.add(tf.keras.layers.Reshape((32, 32, 64)))\n",
        "\n",
        "        model.add(layers.Conv2D(64, kernel_size=4, padding=\"same\"))\n",
        "        model.add(layers.BatchNormalization(momentum=0.8))\n",
        "        model.add(tf.keras.layers.Activation(\"relu\"))\n",
        "        model.add(tf.keras.layers.Dropout(0.2))\n",
        "        model.add(tf.keras.layers.UpSampling2D())\n",
        "        \n",
        "        model.add(layers.Conv2D(32, kernel_size=4, padding=\"same\"))\n",
        "        model.add(layers.BatchNormalization(momentum=0.8))\n",
        "        model.add(tf.keras.layers.Activation(\"relu\"))\n",
        "        model.add(tf.keras.layers.UpSampling2D())\n",
        "\n",
        "        model.add(layers.Conv2D(32, kernel_size=4, padding=\"same\"))\n",
        "        model.add(layers.BatchNormalization(momentum=0.8))\n",
        "        model.add(tf.keras.layers.Activation(\"relu\"))\n",
        "\n",
        "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\", \n",
        "                        activation=\"sigmoid\", name=\"generator_output\"))\n",
        "\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img, name=\"generator\")   \n",
        "\n",
        "    # ================================ discriminator\n",
        "\n",
        "    # The discriminator is a CNN-based image classifier.\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(layers.Conv2D(32, kernel_size=3, strides=2, \n",
        "                                input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(layers.LeakyReLU(alpha=0.2))\n",
        "\n",
        "        model.add(layers.Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(tf.keras.layers.ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "        model.add(layers.BatchNormalization(momentum=0.8))\n",
        "        model.add(layers.LeakyReLU(alpha=0.2))\n",
        "\n",
        "        model.add(layers.Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(layers.BatchNormalization(momentum=0.8))\n",
        "        model.add(layers.LeakyReLU(alpha=0.2))\n",
        "        model.add(layers.Dropout(0.25))\n",
        "\n",
        "        model.add(layers.Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(layers.BatchNormalization(momentum=0.8))\n",
        "        model.add(layers.LeakyReLU(alpha=0.2))\n",
        "        model.add(layers.Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "\n",
        "        #model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        # model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        discrim = Model(img, validity)\n",
        "\n",
        "        discrim.compile(loss='binary_crossentropy',\n",
        "            optimizer=self.optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        return discrim\n",
        "\n",
        "    #   *************\n",
        "    #   train_loop\n",
        "    #\n",
        "    def train_loop(self, \n",
        "                     X_train, \n",
        "                     epochs, \n",
        "                     batch_size=128, \n",
        "                     epochs_batch = 10,\n",
        "                     save_interval=100):\n",
        "\n",
        "        model = self\n",
        "\n",
        "        ckpt = exchecker.get_checkpoint() # get model from self\n",
        "        mgr = exchecker.get_manager(ckpt)\n",
        "        ckptloadstatus = ckpt.restore(mgr.latest_checkpoint)\n",
        "\n",
        "        print(\"ckpt type %s\" %type(ckpt))\n",
        "        print(\"mgr type %s\" %type(mgr))\n",
        "        print(\"ckptloadstatus %s\" %ckptloadstatus)\n",
        "\n",
        "        if mgr.latest_checkpoint:\n",
        "            print(\"Restored from {}\".format(mgr.latest_checkpoint))\n",
        "        else:\n",
        "            print(\"Initializing from scratch.\")\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            \n",
        "            ckpt.qepoch.assign_add(tf.cast(1, tf.float32))\n",
        "            tf.py_function(mgr.save, [], [tf.string])\n",
        "            qepoch = int(model.qepoch.numpy())\n",
        "            print(\"epoch in run: %s; epoch in model: %s\" %(epoch, qepoch))\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random half of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            # Sample noise and generate a batch of new images\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator (real classified as ones and generated as zeros)\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Train the generator (wants discriminator to mistake images as real)\n",
        "            g_loss = self.dcgan.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch % save_interval == 0:\n",
        "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % save_interval == 0:\n",
        "                print(\"save trace image\")\n",
        "                self.save_imgs( \"img_{:05d}.png\".format(qepoch) ) # pass just name\n",
        "  \n",
        "\n",
        "    #   *************\n",
        "    #   __train_loop\n",
        "    #\n",
        "    def __train_loop(self,\n",
        "                   images,\n",
        "                   epochs = 50,\n",
        "                   batch_size = 32,\n",
        "                   save_interval = 100,\n",
        "                   epochs_batch = 10,\n",
        "                   num_examples_to_generate = 16):\n",
        "        \n",
        "        print(\"train_loop - epochs %s, batch_size: %s\" %(epochs, batch_size))\n",
        "\n",
        "        ckpt_prefix = Exchecker.get_ckpt_prefix()\n",
        "        ckpt = Exchecker.get_checkpoint()\n",
        "        mgr = Exchecker.get_manager(ckpt)\n",
        "        print(\"train loop get mgr\")\n",
        "        ckptloadstatus = ckpt.restore(mgr.latest_checkpoint)\n",
        "        print(\"ckptloadstatus %s\" %ckptloadstatus)\n",
        "\n",
        "        if mgr.latest_checkpoint:\n",
        "            print(\"Restored from {}\".format(mgr.latest_checkpoint))\n",
        "            self.latestchkpt = mgr.latest_checkpoint\n",
        "        else:\n",
        "            print(\"Initializing from scratch.\")\n",
        "\n",
        "        num_examples_to_generate = 16\n",
        "        # to visualize progress in the animated GIF)\n",
        "        self.seed = tf.random.normal([num_examples_to_generate, self.latent_dim])\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            ckpt.qepoch.assign_add(tf.cast(1, tf.float32))\n",
        "            tf.py_function(mgr.save, [], [tf.string])\n",
        "            print(\"epoch in run: %s; qepoch in model: %s\" %(epoch, self.qepoch + 0))\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            # =========================\n",
        "\n",
        "            self.train_step(images, epoch, batch_size, save_interval, epochs_batch)\n",
        "\n",
        "            # =========================\n",
        "\n",
        "            # Save the model every nepochs\n",
        "            if (epoch + 1) % epochs_batch == 0:\n",
        "                ckpt.save()\n",
        "\n",
        "            print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "            rvnepoch = self.qepoch\n",
        "            tf.dtypes.cast(rvnepoch, tf.int32) \n",
        "\n",
        "            nepoch = rvnepoch.numpy()\n",
        "            nepoch = nepoch.astype(int)\n",
        "            print(\"rvnepoch rvnepoch value %s type %s \" %(rvnepoch, type(rvnepoch)))\n",
        "\n",
        "            if (epoch + 1) % save_interval == 0:\n",
        "                display.clear_output(wait=True)\n",
        "                self.generate_and_save_images(nepoch, self.seed)        \n",
        "    \n",
        "    #   **************\n",
        "    #   train_step\n",
        "    #\n",
        "    #   Notice the annotation `tf.function`\n",
        "    #   This annotation causes the function to be \"compiled\".\n",
        "    #\n",
        "    @tf.function\n",
        "    def train_step(self, \n",
        "                   X_train, \n",
        "                   epoch, \n",
        "                   batch_size, \n",
        "                   save_interval,\n",
        "                   epochs_batch,\n",
        "                   verbose = False):\n",
        "\n",
        "        if verbose:\n",
        "            print(\"train_step - X_train type: %s\" %type(X_train))\n",
        "            print(\"train_step - img shape: %s\" %X_train[0].shape)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        # Select a random half of images\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        imgs = X_train[idx]\n",
        "\n",
        "        # Sample noise and generate a batch of new images\n",
        "        noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        print(\"noise shape:\", noise.shape)\n",
        "\n",
        "        d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "        d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Generator\n",
        "        # ---------------------\n",
        "\n",
        "        # Train the generator (wants discriminator to mistake images as real)\n",
        "        g_loss = self.dcgan.train_on_batch(noise, valid)\n",
        "\n",
        "        # Plot the progress\n",
        "        if epoch % epochs_batch == 0:\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "    #   ***********\n",
        "    #   Generate and save images\n",
        "    #\n",
        "    def generate_and_save_images(self,\n",
        "                                 epoch, \n",
        "                                 test_input):\n",
        "\n",
        "        model = self.generator \n",
        "        tracepath = self.tracepath\n",
        "\n",
        "        print(\"aa2 generate_and_save_images for epoch %s\" %epoch)\n",
        "\n",
        "        # Notice `training` is set to False.\n",
        "        # This is so all layers run in inference mode (batchnorm).\n",
        "        # predictions = model(test_input, training=False)\n",
        "        predictions = model.predict(test_input)\n",
        "\n",
        "        fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "        try:\n",
        "            gwork\n",
        "        except NameError:\n",
        "            gwork = \"\" # if not exist assume no g prefix\n",
        "\n",
        "        print(\"img trace prefix: %s\" %tracepath)\n",
        "        path = tracepath + '/' + 'image_at_epoch_{:04d}.png'.format(epoch)\n",
        "\n",
        "        for i in range(predictions.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5)\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.savefig(path)\n",
        "        plt.show()\n",
        "\n",
        "    #   ***********\n",
        "    #   Generate and save images\n",
        "    #\n",
        "    def save_imgs(self, name='img.png'):\n",
        "        print(\"aa2 save_imgs for name %s\" %name)\n",
        "        tracepath = self.tracepath\n",
        "\n",
        "        latent_dim = self.latent_dim\n",
        "        predict = self.generator.predict\n",
        "\n",
        "        r, c = 4, 4\n",
        "        noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "\n",
        "        # replace the first two latent variables with known values\n",
        "        #for i in range(r):\n",
        "        #    for j in range(c):\n",
        "        #        noise[4*i+j][0] = i/(r-1)-0.5\n",
        "        #        noise[4*i+j][1] = j/(c-1)-0.5\n",
        "\n",
        "        gen_imgs = predict(noise)\n",
        "\n",
        "        fig, axs = plt.subplots(r, c, figsize=(6.72,6.72))\n",
        "        plt.subplots_adjust(left=0.05,bottom=0.05,right=0.95,top=0.95,wspace=0.2,hspace=0.2)\n",
        "\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt])\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "\n",
        "        fig.savefig(tracepath + '/' + '{}'.format(name) )\n",
        "\n",
        "        plt.close()\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQCwYxeGXEHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#   *****************************\n",
        "#\n",
        "#   TRAIN\n",
        "#\n",
        "if (1 or MODITEM == 'ART2GAN'):\n",
        "\n",
        "    CLEARTMP = 0 # clear g tmp folder \n",
        "\n",
        "    from PIL import Image\n",
        "    from shutil import copyfile\n",
        "\n",
        "    datagen = DATAGEN()\n",
        "    Imagine = IMAGINE()\n",
        "\n",
        "    gsrcdir = gdata + \"nasag/\"\n",
        "    print(\"gsrcdir: %s\" %gsrcdir)\n",
        "\n",
        "    URD_A = 'http://images-assets.nasa.gov/image/PIA03096/'\n",
        "    URFS_A = [ \"PIA03096~orig.jpg\",  ]\n",
        "\n",
        "    wdstdir = gwork + \"nasag/\"             \n",
        "    rawimgdir = wdstdir\n",
        "\n",
        "    print(\"gsrcdir %s \" %gsrcdir)\n",
        "    print(\"wdstdir %s \" %wdstdir)\n",
        "    \n",
        "    datagen.get_uriitems(URD_A, URFS_A, gsrcdir, wdstdir)\n",
        "\n",
        "    x_train, y_train = datagen.create_dataset_np(128, 128, \n",
        "                                    nSlices=1000, \n",
        "                                    resize=0.1, \n",
        "                                    directory=rawimgdir) #  datagen.create_dataset_np\n",
        "    shape = x_train.shape\n",
        "    print(\"shape: \", shape) # (1000, 128, 128, 3)\n",
        "    print(\"shape[0]: \", shape[0])\n",
        "    assert(shape[0]>0)\n",
        "\n",
        "    # plot results from datagen\n",
        "    if 1:\n",
        "        fig, axs = plt.subplots(4, 4)\n",
        "        for i in range(4):\n",
        "            for j in range(4):\n",
        "                axs[i,j].imshow( x_train[ np.random.randint(x_train.shape[0]) ] )\n",
        "                axs[i,j].axis('off')\n",
        "        plt.show()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNvpHufyamyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if (1 or MODITEM == 'ART2GAN'):\n",
        "    # instance model\n",
        "\n",
        "    prefix = gwork\n",
        "    mname = 'aa2'\n",
        "    imgdir = 'imgdir' # img holder \n",
        "    ckptdir  = 'ckptdir' # checkpoints place\n",
        "    tracedir  = 'tracedir' # in loop folder   \n",
        "\n",
        "    modpath = prefix + '/' + mname\n",
        "    imgpath = modpath + '/' + imgdir\n",
        "    ckptpath = modpath + '/' + ckptdir\n",
        "    tracepath = modpath + '/' + tracedir\n",
        "\n",
        "    if not os.path.exists(modpath ):\n",
        "        os.mkdir(modpath )     \n",
        "    if not os.path.exists(imgpath ):\n",
        "        os.mkdir(imgpath )        \n",
        "    if not os.path.exists(ckptpath ):\n",
        "        os.mkdir(ckptpath )        \n",
        "    if not os.path.exists(tracepath ):\n",
        "        os.mkdir(tracepath )        \n",
        "\n",
        "    art2gan = ART2GAN(img_rows = x_train[0].shape[0],\n",
        "                    img_cols = x_train[0].shape[1],\n",
        "                    channels = x_train[0].shape[2], \n",
        "                    latent_dim = 256,\n",
        "                    imgpath = imgpath, # img holder \n",
        "                    ckptpath  = ckptpath, # checkpoints place\n",
        "                    tracepath  = tracepath, # in loop folder                    \n",
        "                    ) # ?\n",
        "\n",
        "if (1 or MODITEM == 'ART2GAN'):\n",
        "    exchecker = EXCHECKER(art2gan,\n",
        "                          clear = False)  \n",
        "if (1 or MODITEM == 'ART2GAN'):\n",
        "    # try:\n",
        "    #     art2gan.load_weights(generator_file=\"generator ({}).h5\".format(art2gan.mname), discriminator_file=\"discriminator ({}).h5\".format(art2gan.name) )\n",
        "    # except:\n",
        "    #     pass       \n",
        "\n",
        "\n",
        "    art2gan.train_loop(x_train, \n",
        "                epochs=110, \n",
        "                batch_size=32, \n",
        "                save_interval=10,\n",
        "                epochs_batch = 10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIk99bjRV8sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ANIME\n",
        "\n",
        "anime = ANIME(art2gan)\n",
        "gif = anime.generate_gif(srcfolder = tracepath, dstfile = 'ani.gif')         \n",
        "anime.display_gif(giffile='ani.gif')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riazTwOAj6xD",
        "colab_type": "text"
      },
      "source": [
        "# === BIGAN\n",
        "\n",
        "https://github.com/manicman1999/Keras-BiGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhPpNT1kHyIf",
        "colab_type": "text"
      },
      "source": [
        "## MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVs91ISu5xyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "from math import floor\n",
        "import numpy as np\n",
        "import time\n",
        "from functools import partial\n",
        "from random import random\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, Dense, AveragePooling2D, Activation, Cropping2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Reshape, UpSampling2D, Flatten, Input, add, Lambda, concatenate, LeakyReLU, multiply\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, average\n",
        "from tensorflow.keras.models import model_from_json, Model\n",
        "from tensorflow.keras.initializers import VarianceScaling\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from PIL import Image\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "im_size = 128\n",
        "latent_size = 64\n",
        "BATCH_SIZE = 32\n",
        "cmode = 'RGB'\n",
        "channels = 3\n",
        "size_adjusted = False\n",
        "\n",
        "directory = \"Faces256\"\n",
        "suff = 'jpg'\n",
        "\n",
        "datafolder = \"bigan/\"\n",
        "if not os.path.exists(datafolder):\n",
        "    os.mkdir(datafolder)\n",
        "\n",
        "resultsfolder = \"Results/\"\n",
        "if not os.path.exists(resultsfolder):\n",
        "    os.mkdir(resultsfolder)\n",
        "\n",
        "modelsfolder = \"Models/\"\n",
        "if not os.path.exists(modelsfolder):\n",
        "    os.mkdir(modelsfolder)\n",
        "\n",
        "rawimgdir = datafolder + \"images/\"\n",
        "if not os.path.exists(rawimgdir):\n",
        "    os.mkdir(rawimgdir)\n",
        "\n",
        "\n",
        "srcpath = datafolder + directory + str(im_size)\n",
        "print(f\"srcpath: {srcpath}\")\n",
        "\n",
        "URD_A = 'https://www.nasa.gov/sites/default/files/styles/full_width_feature/public/thumbnails/image/'\n",
        "URFS_A = [ \"potw1934a.jpg\", \"potw1935a.jpg\", \"potw1936a.jpg\", \"potw1937a.jpg\", \"potw1938a.jpg\",\n",
        "        \"potw1939a.jpg\", \"potw1940a.jpg\", \"potw1941a.jpg\", \"potw1942a.jpg\", \"potw1943a.jpg\" ]\n",
        "\n",
        "gdir_a = gdata + '/nasa'\n",
        "wdir_a = gwork + rawimgdir\n",
        "\n",
        "datagen = DATAGEN()\n",
        "datagen.get_uriitems(URD_A, URFS_A, gdir_a, wdir_a)\n",
        "\n",
        "\n",
        "class BIGAN(object):\n",
        "\n",
        "    def __init__(self, steps = 1, lr = 0.0001, decay = 0.00001):\n",
        "\n",
        "        #Models\n",
        "        self.D = None\n",
        "        self.E = None\n",
        "        self.G = None\n",
        "\n",
        "        self.GE = None\n",
        "        self.EE = None\n",
        "\n",
        "        self.DM = None\n",
        "        self.AM = None\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J87GCOnylYZG",
        "colab_type": "text"
      },
      "source": [
        "## DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDim4JIXlaTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 0:\n",
        "    from PIL import Image\n",
        "    from shutil import copyfile\n",
        "\n",
        "\n",
        "    # datagen = DATAGEN()\n",
        "    # Imagine = IMAGINE()\n",
        "\n",
        "    datafolder = \"data/\"\n",
        "    if not os.path.exists(datafolder):\n",
        "        os.mkdir(datafolder)\n",
        "\n",
        "    resultsfolder = \"Results/\"\n",
        "    if not os.path.exists(resultsfolder):\n",
        "        os.mkdir(resultsfolder)\n",
        "\n",
        "    modelsfolder = \"Models/\"\n",
        "    if not os.path.exists(modelsfolder):\n",
        "        os.mkdir(modelsfolder)\n",
        "\n",
        "    rawimgdir = \"data/images/\"\n",
        "    if not os.path.exists(rawimgdir):\n",
        "        os.mkdir(rawimgdir)\n",
        "\n",
        "\n",
        "    srcdir = \"nasag/\"\n",
        "    gsrcdir = gdata + srcdir\n",
        "    print(\"gsrcdir: %s\" %gsrcdir)\n",
        "\n",
        "\n",
        "    URD_A = 'https://www.nasa.gov/sites/default/files/styles/full_width_feature/public/thumbnails/image/'\n",
        "    URFS_A = [\n",
        "            \"potw1934a.jpg\", \n",
        "            \"potw1935a.jpg\", \n",
        "            \"potw1936a.jpg\",\n",
        "            \"potw1937a.jpg\",\n",
        "            \"potw1938a.jpg\",\n",
        "            \"potw1939a.jpg\",\n",
        "            \"potw1940a.jpg\",\n",
        "            \"potw1941a.jpg\",\n",
        "            \"potw1942a.jpg\",\n",
        "            \"potw1943a.jpg\",\n",
        "            \"potw1944a.jpg\",\n",
        "            ]\n",
        "    DIR_A = rawimgdir\n",
        "\n",
        "\n",
        "    if 1: # if g src eq g raw, download to raw\n",
        "        dstdir = rawimgdir\n",
        "        if not os.path.exists(dstdir):\n",
        "            print(\"create imgfolder: %s\" %dstdir)\n",
        "            os.mkdir(dstdir)\n",
        "    \n",
        "        if os.path.exists(gsrcdir):\n",
        "            for file in URFS_A:\n",
        "                srcfile = os.path.join(gsrcdir, file)\n",
        "                dstfile = os.path.join(dstdir, file)\n",
        "                if os.path.exists(srcfile) and not os.path.exists(dstfile):\n",
        "                    print(\"copy from gdrive: cp %s to %s\" %(srcfile, dstfile))\n",
        "                    copy(srcfile, dstfile)\n",
        "\n",
        "                else:\n",
        "                    print(\"download from uri %s\" %file)\n",
        "                    uri = URD_A + file\n",
        "                    !wget --no-check-certificate $uri -P  \"$dstdir\"\n",
        "\n",
        "        else:\n",
        "            for img in URFS_A:\n",
        "                print(img)\n",
        "                uri = URD_A + img\n",
        "                !wget --no-check-certificate $uri -P  $dstdir\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8roGTJ-nkwj2",
        "colab_type": "text"
      },
      "source": [
        "## TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuKYgmpXkvUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 0:\n",
        "    model = BiGAN(lr = 0.0001, silent = False)\n",
        "    model.evaluate(0)\n",
        "\n",
        "    while model.GAN.steps <= 600000:\n",
        "        model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOCGf1zqlBy7",
        "colab_type": "text"
      },
      "source": [
        "## EVAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj260AVulFE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 0:\n",
        "    from idw import IDW\n",
        "    from keras.models import model_from_json\n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    import os\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    from sklearn.cluster import KMeans\n",
        "    import random\n",
        "    import time\n",
        "\n",
        "    directory = \"Earth\"\n",
        "    im_size = 128\n",
        "    suffix = 'jpg'\n",
        "    model_num = 15\n",
        "\n",
        "    def loadModel(mname, num):\n",
        "\n",
        "        file = open(\"Models/\"+mname+\".json\", 'r')\n",
        "        json = file.read()\n",
        "        file.close()\n",
        "\n",
        "        mod = model_from_json(json)\n",
        "        mod.load_weights(\"Models/\"+mname+\"_\"+str(num)+\".h5\")\n",
        "\n",
        "        return mod\n",
        "\n",
        "    class dataGenerator(object):\n",
        "\n",
        "        def __init__(self, loc, flip = True, suffix = 'png'):\n",
        "            self.flip = flip\n",
        "            self.suffix = suffix\n",
        "            self.files = []\n",
        "            self.n = 1e10\n",
        "\n",
        "            print(\"Importing Images...\")\n",
        "\n",
        "            try:\n",
        "                os.mkdir(\"data/\" + loc + \"-npy-\" + str(im_size))\n",
        "            except:\n",
        "                self.load_from_npy(loc)\n",
        "                return\n",
        "\n",
        "            for dirpath, dirnames, filenames in os.walk(\"data/\" + loc):\n",
        "                for filename in [f for f in filenames if f.endswith(\".\"+str(self.suffix))]:\n",
        "                    print('\\r' + str(len(self.files)), end = '\\r')\n",
        "                    fname = os.path.join(dirpath, filename)\n",
        "                    temp = Image.open(fname).convert(cmode)\n",
        "                    if not size_adjusted:\n",
        "                        temp = temp.resize((im_size, im_size), Image.BILINEAR)\n",
        "                    temp = np.array(temp, dtype='uint8')\n",
        "                    self.files.append(temp)\n",
        "                    if self.flip:\n",
        "                        self.files.append(np.flip(temp, 1))\n",
        "\n",
        "            self.files = np.array(self.files)\n",
        "            np.save(\"data/\" + loc + \"-npy-\" + str(im_size) + \"/data.npy\", self.files)\n",
        "\n",
        "            self.n = self.files.shape[0]\n",
        "\n",
        "            print(\"Found \" + str(self.n) + \" images in \" + loc + \".\")\n",
        "\n",
        "        def load_from_npy(self, loc):\n",
        "\n",
        "            print(\"Loading from .npy files.\")\n",
        "\n",
        "            self.files = np.load(\"data/\" + str(loc) + \"-npy-\" + str(im_size) + \"/data.npy\")\n",
        "\n",
        "            self.n = self.files.shape[0]\n",
        "\n",
        "\n",
        "        def get_batch(self, num):\n",
        "\n",
        "            idx = np.random.randint(0, self.n, num)\n",
        "            out = []\n",
        "\n",
        "            for i in range(num):\n",
        "                out.append(self.files[idx[i]])\n",
        "\n",
        "            return np.array(out).astype('float32') / 255.0\n",
        "\n",
        "    data = dataGenerator(directory, suffix = suffix)\n",
        "    encoder = loadModel(\"enc\", model_num)\n",
        "    generator = loadModel(\"gen\", model_num)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkSQZoEhKDXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}